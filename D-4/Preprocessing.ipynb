{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edace149",
   "metadata": {},
   "source": [
    "Pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b76daa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Funcs.Utility import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, Callable, Union, Tuple, List, Optional, Iterable\n",
    "from datetime import timedelta as td\n",
    "from scipy import stats\n",
    "import ray\n",
    "import warnings\n",
    "import time\n",
    "import ray\n",
    "import dask\n",
    "import scipy.spatial.distance as dist\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb21f91",
   "metadata": {},
   "source": [
    "New modified Functions based on Panyu's original D1 preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "687e43f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from poi import PoiCluster\n",
    "import googlemaps\n",
    "import warnings\n",
    "from pandas.errors import PerformanceWarning\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=PerformanceWarning)\n",
    "warnings.simplefilter(action=\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "# Define the center and radius of the circle around KAIST main campus\n",
    "center_lat_kaist, center_lon_kaist = (36.3722, 127.3600)\n",
    "_radius_kaist = 1000  # meters\n",
    "\n",
    "# Define the center and radius of the circle around KAIST Munji campus\n",
    "center_lat_munji, center_lon_munji = (36.391944, 127.398611)\n",
    "_radius_munji = 400  # meters\n",
    "\n",
    "# Define the center and radius of the circle around KAIST Seoul campus\n",
    "center_lat_seoul, center_lon_seoul = (37.5933, 127.0464)\n",
    "_radius_seoul = 300  # meters\n",
    "\n",
    "\n",
    "def _haversine(_lat1, _lat2, _lng1, _lng2) -> float:\n",
    "    if np.isnan(_lat1) or np.isnan(_lat2) or np.isnan(_lng1) or np.isnan(_lng2):\n",
    "        return 0.0\n",
    "\n",
    "    _lat1_r, _lat2_r, _lng1_r, _lng2_r = np.radians(_lat1), np.radians(_lat2), np.radians(_lng1), np.radians(_lng2)\n",
    "    _lat = _lat2_r - _lat1_r\n",
    "    _lng = _lng2_r - _lng1_r\n",
    "    _R = 6371008.8\n",
    "    _d = np.sin(_lat * 0.5) ** 2 + np.cos(_lat1_r) * np.cos(_lat2_r) * np.sin(_lng * 0.5) ** 2\n",
    "    return 2 * _R * np.arcsin(np.sqrt(_d))\n",
    "\n",
    "# Calculate the distances between the cluster centers and the center of the circle\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    R = 6371000 # meters\n",
    "    phi1 = math.radians(lat1)\n",
    "    phi2 = math.radians(lat2)\n",
    "    delta_phi = math.radians(lat2 - lat1)\n",
    "    delta_lambda = math.radians(lon2 - lon1)\n",
    "    a = math.sin(delta_phi/2)**2 + \\\n",
    "        math.cos(phi1)*math.cos(phi2)*math.sin(delta_lambda/2)**2\n",
    "    c = 2*math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "    d = R*c\n",
    "    return d\n",
    "\n",
    "#Weiszfeld algorithm to calculate midpoint in a cluster\n",
    "def midpoint(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Load the GPS locations of the cluster into a NumPy array\n",
    "    _data = np.array(data[['latitude','longitude']])\n",
    "    # Set the initial estimate to the mean of the GPS coordinates\n",
    "    midpoint = np.mean(_data, axis=0)\n",
    "    # Define the stopping criterion\n",
    "    epsilon = 1e-6\n",
    "    # Define the maximum number of iterations\n",
    "    max_iterations = 100\n",
    "    # Define the Weiszfeld algorithm\n",
    "    for i in range(max_iterations):\n",
    "        # Compute the distances between the midpoint and the points\n",
    "        distances = np.sqrt(np.sum((_data - midpoint)**2, axis=1))\n",
    "        \n",
    "        # Check if any distance is 0\n",
    "        if np.any(distances == 0):\n",
    "            idx = np.where(distances == 0)[0][0]\n",
    "            return pd.DataFrame({'mid_latitude': _data[idx, 0], 'mid_longitude': _data[idx, 1]}, index=data.index)\n",
    "       \n",
    "        \n",
    "        # Check if the stopping criterion has been reached\n",
    "        if np.max(distances) < epsilon:\n",
    "            break\n",
    "        # Compute the weighted mean of the GPS coordinates\n",
    "        weights = 1.0 / distances\n",
    "        midpoint = np.sum(_data * weights[:, np.newaxis], axis=0) / np.sum(weights)\n",
    "    return pd.DataFrame({'mid_latitude': midpoint[0], 'mid_longitude': midpoint[1]}, index=data.index)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123dfc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the API key and client\n",
    "API_KEY = \"YOUR_API_KEY\"\n",
    "client = googlemaps.Client(API_KEY)\n",
    "\n",
    "Eating = ['restaurant']\n",
    "Social = ['bar','cafe','movie_theater','night_club']\n",
    "Gym = ['gym']\n",
    "known = Eating + Social + Gym\n",
    "\n",
    "def label_cluster(data, radius):\n",
    "    location = (data['mid_latitude'].mean(),data['mid_longitude'].mean())\n",
    "    #return data['mid_latitude'].mean()\n",
    "    # Perform the search\n",
    "    results = client.places_nearby(location=location, radius=radius)\n",
    "    # Filter the results by type (e.g. restaurant)\n",
    "    known_places = [place for place in results['results'] if any(elem in place['types'] for elem in known)]\n",
    "    if known_places:\n",
    "        # Get the closest place to the location\n",
    "        closest_place = min(known_places, key=lambda p: p.get('distance', {}).get('value', float('inf')))\n",
    "\n",
    "        if any(elem in closest_place['types'] for elem in Social):\n",
    "            data['label']='social'\n",
    "        elif any(elem in closest_place['types'] for elem in Eating):\n",
    "            data['label']='eating'\n",
    "        else:\n",
    "            data['label']='gym'\n",
    "    else:\n",
    "        data['label']='others'\n",
    "    return data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "992188eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ActivityEvent.csv (O)\n",
    "def _proc_activity_event(data: pd.DataFrame) -> Union[pd.Series, Dict[str, pd.Series]]:\n",
    "    return {\n",
    "        'UNK': data['confidenceUnknown'].astype('float32'),\n",
    "        'FOT': data['confidenceOnFoot'].astype('float32'),\n",
    "        'WLK': data['confidenceWalking'].astype('float32'),\n",
    "        'VHC': data['confidenceInVehicle'].astype('float32'),\n",
    "        'BCC': data['confidenceOnBicycle'].astype('float32'),\n",
    "        'RUN': data['confidenceRunning'].astype('float32'),\n",
    "        'TLT': data['confidenceTilting'].astype('float32')\n",
    "    }\n",
    "\n",
    "# ActivityTransition.csv (O)\n",
    "def _proc_activity_transition(data: pd.DataFrame) -> Union[pd.Series, Dict[str, pd.Series]]:\n",
    "    data = data.loc[\n",
    "        lambda x: x['typeEnter'].isin(['WALKING', 'RUNNING', 'ON_BICYCLE', 'STILL', 'IN_VEHICLE']), :\n",
    "    ]\n",
    "    return data['typeEnter'].astype('object')\n",
    "\n",
    "def _proc_wireless_state(data: pd.DataFrame) -> Union[pd.Series, Dict[str, pd.Series]]:\n",
    "    return data['type'].astype('object')\n",
    "\n",
    "# FitnessActivity.csv (O)\n",
    "def _proc_activity(data: pd.DataFrame) -> Union[pd.Series, Dict[str, pd.Series]]:\n",
    "    return {\n",
    "        'VAL': data['value'].astype('object')\n",
    "    }\n",
    "\n",
    "# FitnessStepCount.csv (O)\n",
    "def _proc_step_count(data: pd.DataFrame) -> Union[pd.Series, Dict[str, pd.Series]]:\n",
    "    return {\n",
    "        'VAL': data['value'].astype('int')\n",
    "    }\n",
    "\n",
    "# FitnessDistance.csv (O)\n",
    "def _proc_distance(data: pd.DataFrame) -> Union[pd.Series, Dict[str, pd.Series]]:\n",
    "    return {\n",
    "        'VAL': data['value'].astype('float32')\n",
    "    }\n",
    "\n",
    "# FitnessCalorie.csv (O)\n",
    "def _proc_calorie(data: pd.DataFrame) -> Union[pd.Series, Dict[str, pd.Series]]:\n",
    "\n",
    "    return {\n",
    "        'VAL': data['value'].astype('float32')\n",
    "    }\n",
    "\n",
    "def _proc_app_usage(data: pd.DataFrame) -> Union[pd.Series, Dict[str, pd.Series]]:\n",
    "    # Filter the data to include only relevant types of app usage events\n",
    "    data = data.loc[\n",
    "        lambda x: x['type'].isin(['ACTIVITY_RESUMED', 'ACTIVITY_PAUSED']), :\n",
    "    ].assign(\n",
    "        category=lambda x: np.where(x['type'] == 'ACTIVITY_RESUMED', x['category'], None),\n",
    "    )\n",
    "    \n",
    "    data = data.rename(columns={'category': 'subcategory'})\n",
    "    data['category'] = [transform.get(item, item) for item in data['subcategory'].values]\n",
    "    \n",
    "    move = data\n",
    "    Duration = []\n",
    "    for pcode in data.index.get_level_values('pcode').unique():\n",
    "        sub_move = move.loc[(pcode, ), :].sort_index(axis=0, level='timestamp').assign(pcode=pcode)\n",
    "        sub_move = sub_move.reset_index()\n",
    "        sub_move['move_state'] = sub_move['type'].shift().fillna('ACTIVITY_PAUSED')\n",
    "        sub_move.loc[0, 'move_state'] = 'ACTIVITY_PAUSED'\n",
    "        sub_move = sub_move[sub_move['move_state'] != sub_move['type']]\n",
    "        sub_move.index = pd.to_datetime(sub_move.index)\n",
    "        sub_move['duration'] = sub_move['timestamp'] - sub_move['timestamp'].shift()\n",
    "        sub_move.loc[0, 'duration'] = pd.Timedelta(0)\n",
    "        sub_move = sub_move[sub_move['duration'] > pd.Timedelta(0)]\n",
    "        sub_move['duration_sec'] = sub_move['duration'].dt.total_seconds()\n",
    "        sub_move['category'] = sub_move['category']\n",
    "        Duration.append(sub_move)\n",
    "    \n",
    "    Duration = pd.concat(Duration, axis=0, ignore_index=True).set_index(\n",
    "        ['pcode', 'timestamp']\n",
    "    )\n",
    "    Duration = Duration[Duration['type'] == 'ACTIVITY_RESUMED']\n",
    "    \n",
    "    cnt = Duration['category'].value_counts()\n",
    "    _val, _sup = cnt.index, cnt.values\n",
    "    \n",
    "    DUR = {'DUR_{}'.format(_k): Duration[Duration['category'] == '{}'.format(_k)]['duration_sec'].astype('float32') for _k in _val}\n",
    "    \n",
    "    CAT = {'CAT': data['category'].astype('object')}\n",
    "    \n",
    "    Feature = {**DUR, **CAT}\n",
    "    \n",
    "    return Feature\n",
    "\n",
    "\n",
    "def _proc_battery(data: pd.DataFrame) -> Union[pd.Series, Dict[str, pd.Series]]:\n",
    "    return {\n",
    "        'LEV': data['level'].astype('float32'),\n",
    "        'STA': data['status'].astype('object'),\n",
    "        'TMP': (data['temperature'] / 10).astype('float32'),  # Assuming temperature is now in deci-degrees\n",
    "        'PLG': data['pluggedType'].astype('object')\n",
    "    }\n",
    "\n",
    "\n",
    "def _proc_call(data: pd.DataFrame) -> Union[pd.Series, Dict[str, pd.Series]]:\n",
    "    # Filter out calls with duration = 0\n",
    "    data = data.loc[\n",
    "        lambda x: x['duration'] > 0, :\n",
    "    ]\n",
    "\n",
    "    return {\n",
    "        'DUR': data['duration'].astype('float32'),\n",
    "        'CNT': data['contactType'].astype('object')\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "def _proc_data_traffic(data: pd.DataFrame) -> Union[pd.Series, Dict[str, pd.Series]]:\n",
    "    return {\n",
    "        'RCV': data['rxBytes'].astype('float32'),\n",
    "        'SNT': data['txBytes'].astype('float32'),\n",
    "        'MRCV': data['mobileRxBytes'].astype('float32'),\n",
    "        'MSNT': data['mobileTxBytes'].astype('float32')\n",
    "    }\n",
    "\n",
    "def _proc_ringer_mode(data: pd.DataFrame) -> Union[pd.Series, Dict[str, pd.Series]]:\n",
    "    return data['type'].astype('object')\n",
    "\n",
    "def _proc_screen(data: pd.DataFrame) -> Union[pd.Series, Dict[str, pd.Series]]:\n",
    "    screen_events = data[data['type'].isin(['SCREEN_ON', 'SCREEN_OFF', 'USER_PRESENT'])]\n",
    "    \n",
    "    Duration = []\n",
    "    for pcode in screen_events.index.get_level_values('pcode').unique():\n",
    "        sub_screen = screen_events.loc[(pcode, ), :].sort_index(axis=0, level='timestamp').assign(pcode=pcode)\n",
    "        sub_screen = sub_screen.reset_index()\n",
    "        sub_screen['screen_state'] = sub_screen['type'].shift().fillna('SCREEN_OFF')\n",
    "        sub_screen.loc[0, 'screen_state'] = 'SCREEN_OFF'\n",
    "        sub_screen = sub_screen[sub_screen['screen_state'] != sub_screen['type']]\n",
    "        sub_screen.index = pd.to_datetime(sub_screen.index)\n",
    "        sub_screen['duration'] = sub_screen['timestamp'] - sub_screen['timestamp'].shift()\n",
    "        sub_screen.loc[0, 'duration'] = pd.Timedelta(0)\n",
    "        sub_screen = sub_screen[sub_screen['duration'] > pd.Timedelta(0)]\n",
    "        sub_screen['duration_sec'] = sub_screen['duration'].dt.total_seconds()\n",
    "        Duration.append(sub_screen)\n",
    "    \n",
    "    Duration = pd.concat(Duration, axis=0, ignore_index=True).set_index(['pcode', 'timestamp'])\n",
    "    \n",
    "    return {\n",
    "        'EVENT': screen_events['type'].astype('object'),\n",
    "        'DUR': Duration['duration_sec'].astype('float32')\n",
    "    }\n",
    "\n",
    "def _proc_on_off(data: pd.DataFrame) -> Union[pd.Series, Dict[str, pd.Series]]:\n",
    "    return data['type'].astype('object')\n",
    "\n",
    "def _proc_power_save(data: pd.DataFrame) -> Union[pd.Series, Dict[str, pd.Series]]:\n",
    "    return data['type'].astype('object')\n",
    "\n",
    "def _proc_charge(data: pd.DataFrame) -> Union[pd.Series, Dict[str, pd.Series]]:\n",
    "    return data['type'].astype('object')\n",
    "\n",
    "\n",
    "\n",
    "def _proc_location(data: pd.DataFrame) -> Union[pd.Series, Dict[str, pd.Series]]:\n",
    "    new_data = []\n",
    "    DISTANCE_MAX_IN_METRE = 100  # Maximum distance in meters to be considered the same point of interest (POI)\n",
    "    REGION_SIZE_IN_METRE = 250  # Maximum region size in meters for clustering POIs\n",
    "    MAXIMUM_TIME_IN_MIN = 60  # Maximum time in minutes to be at a location\n",
    "    MINIMUM_TIME_IN_MIN = 5  # Minimum time in minutes to be considered a visit at a location\n",
    "\n",
    "    # Iterate through each unique pcode (user identifier)\n",
    "    for pcode in data.index.get_level_values('pcode').unique():\n",
    "        # Prepare subset of data for the current user\n",
    "        sub = data.loc[(pcode,), :].sort_index(axis=0, level='timestamp').assign(\n",
    "            _latitude=lambda x: x['latitude'].shift(1),  # Shift latitude to get the previous value\n",
    "            _longitude=lambda x: x['longitude'].shift(1),  # Shift longitude to get the previous value\n",
    "            dist=lambda x: x.apply(\n",
    "                lambda y: _haversine(y['latitude'], y['_latitude'], y['longitude'], y['_longitude']),\n",
    "                axis=1  # Calculate distance between current and previous coordinates\n",
    "            ),\n",
    "            pcode=pcode\n",
    "        ).reset_index()\n",
    "\n",
    "        # Convert timestamp to milliseconds\n",
    "        sub['timestamp'] = sub['timestamp'].apply(lambda x: int(pd.Timestamp(x).timestamp() * 1000))\n",
    "        \n",
    "        # Filter out rows with high GPS accuracy values\n",
    "        sub = sub[sub['accuracy'] < 100]\n",
    "\n",
    "        # Convert latitude and longitude to radians for clustering\n",
    "        latlon_rad = np.radians(sub.loc[:, ['latitude', 'longitude']].to_numpy())\n",
    "        timestamps = sub.loc[:, 'timestamp'].values\n",
    "\n",
    "        # Fit the POI clustering model\n",
    "        cluster = PoiCluster(\n",
    "            d_max=DISTANCE_MAX_IN_METRE, r_max=REGION_SIZE_IN_METRE, t_max=MAXIMUM_TIME_IN_MIN * 60 * 1000,\n",
    "            t_min=MINIMUM_TIME_IN_MIN * 60 * 1000\n",
    "        ).fit(X=latlon_rad, timestamps=timestamps)\n",
    "        \n",
    "        # Predict cluster labels for each point\n",
    "        labels = cluster.predict(X=latlon_rad)\n",
    "        sub = sub.assign(cluster=labels)\n",
    "\n",
    "        # Replace empty cluster labels with 'NONE'\n",
    "        sub['cluster'].replace('', 'NONE', inplace=True)\n",
    "\n",
    "        # Calculate midpoint for each cluster\n",
    "        sub[['mid_latitude', 'mid_longitude']] = sub.groupby('cluster', group_keys=False).apply(midpoint).apply(pd.Series)\n",
    "        \n",
    "        # Convert timestamp back to datetime and set time zone\n",
    "        sub = sub.assign(\n",
    "            _timestamp=lambda x: pd.to_datetime(x['timestamp'], unit='ms', utc=True).dt.tz_convert(DEFAULT_TZ)\n",
    "        )\n",
    "        \n",
    "        # Sort the DataFrame by timestamp\n",
    "        sub = sub.sort_values(by='_timestamp')\n",
    "\n",
    "        # # Extract temporal features\n",
    "        # sub['day_of_week'] = sub['_timestamp'].dt.dayofweek  # Day of the week (0-6)\n",
    "        # sub['hour_of_day'] = sub['_timestamp'].dt.hour  # Hour of the day (0-23)\n",
    "        # sub['day_or_night'] = sub['_timestamp'].apply(lambda x: 1 if 9 <= x.hour < 18 else 0)  # Determine if it's day or night\n",
    "        # sub['wkday_or_wkend'] = sub['_timestamp'].apply(lambda x: 1 if x.dayofweek <= 4 else 0)  # Determine if it's a weekday or weekend\n",
    "        # sub['duration'] = sub['_timestamp'].diff().apply(lambda x: x.total_seconds() if isinstance(x, pd.Timedelta) else x)  # Calculate duration between consecutive timestamps\n",
    "        # sub = sub.fillna(0)\n",
    "\n",
    "        # # Identify home and work clusters\n",
    "        # home = sub[sub['day_or_night'] == 0]['duration'].groupby(sub['cluster']).sum().idxmax()  # Cluster with most nighttime duration\n",
    "        # work = sub[sub['day_or_night'] == 1][sub['wkday_or_wkend'] == 1][sub['cluster'] != home]['duration'].groupby(sub['cluster']).sum().idxmax()  # Cluster with most daytime weekday duration (excluding home)\n",
    "\n",
    "        # # Label clusters as 'home', 'work', or 'none'\n",
    "        # condition_home = sub['cluster'] == home\n",
    "        # condition_work = sub['cluster'] == work\n",
    "        # condition_none = sub['cluster'] == 'NONE'\n",
    "        # sub.loc[condition_home, 'label'] = 'home'\n",
    "        # sub.loc[condition_work, 'label'] = 'work'\n",
    "        # sub.loc[condition_none, 'label'] = 'none'\n",
    "\n",
    "        # # Label other clusters based on proximity using a specified radius\n",
    "        # radius = 100\n",
    "        # mask = sub['label'].isna()\n",
    "        # sub['label'] = sub[mask].groupby('cluster', group_keys=False).apply(lambda x: label_cluster(x, radius)).apply(pd.Series)\n",
    "\n",
    "        # # Re-label clusters as 'home', 'work', or 'none'\n",
    "        # condition_home = sub['cluster'] == home\n",
    "        # condition_work = sub['cluster'] == work\n",
    "        # condition_none = sub['cluster'] == 'NONE'\n",
    "        # sub.loc[condition_home, 'label'] = 'home'\n",
    "        # sub.loc[condition_work, 'label'] = 'work'\n",
    "        # sub.loc[condition_none, 'label'] = 'none'\n",
    "\n",
    "        # # Calculate distances to specific known locations (e.g., KAIST, Munji, Seoul)\n",
    "        # centers = sub[sub['label'] == 'others'].groupby('cluster').mean()\n",
    "        # distances_kaist = centers.apply(lambda row: haversine(center_lat_kaist, center_lon_kaist, row['latitude'], row['longitude']), axis=1)\n",
    "        # distances_munji = centers.apply(lambda row: haversine(center_lat_munji, center_lon_munji, row['latitude'], row['longitude']), axis=1)\n",
    "        # distances_seoul = centers.apply(lambda row: haversine(center_lat_seoul, center_lon_seoul, row['latitude'], row['longitude']), axis=1)\n",
    "\n",
    "        # # Determine which clusters fall within specified radii of known locations\n",
    "        # in_circle_kaist = distances_kaist <= _radius_kaist\n",
    "        # cluster_centers_in_circle_kaist = centers[in_circle_kaist]\n",
    "\n",
    "        # in_circle_munji = distances_munji <= _radius_munji\n",
    "        # cluster_centers_in_circle_munji = centers[in_circle_munji]\n",
    "\n",
    "        # in_circle_seoul = distances_seoul <= _radius_seoul\n",
    "        # cluster_centers_in_circle_seoul = centers[in_circle_seoul]\n",
    "\n",
    "        # # Combine clusters within known locations\n",
    "        # cluster_centers_in_circle = pd.concat([cluster_centers_in_circle_kaist, cluster_centers_in_circle_munji, cluster_centers_in_circle_seoul])\n",
    "\n",
    "        # # Label clusters that are within known locations as 'work'\n",
    "        # condition_work_other = sub['cluster'].isin(cluster_centers_in_circle.index)\n",
    "        # sub.loc[condition_work_other, 'label'] = 'work'\n",
    "\n",
    "        # Append processed data for the current user\n",
    "        new_data.append(sub)\n",
    "\n",
    "    # Concatenate data from all users\n",
    "    new_data = pd.concat(new_data, axis=0, ignore_index=True)\n",
    "\n",
    "    # Convert timestamp back to datetime with the default time zone\n",
    "    new_data['timestamp'] = pd.to_datetime(new_data['timestamp'], unit='ms', utc=True).dt.tz_convert(DEFAULT_TZ)\n",
    "    new_data = new_data.set_index(['pcode', 'timestamp'])\n",
    "\n",
    "    # Return the labeled location data\n",
    "    return {\n",
    "        # 'LABEL': new_data['label'].astype('object'),\n",
    "        'DST': new_data['dist'].astype('float32'),\n",
    "        'CLS': new_data['cluster'].astype('object'),\n",
    "\n",
    "        \n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _proc_wifi(data: pd.DataFrame) -> Union[pd.Series, Dict[str, pd.Series]]:\n",
    "    new_data = []\n",
    "\n",
    "    for pcode in data.index.get_level_values('pcode').unique():\n",
    "        sub = data.loc[(pcode,), :].sort_index(axis=0, level='timestamp').assign(\n",
    "            bssid=lambda x: x['address'].str.cat(x['frequency'].astype(str), sep='-')\n",
    "        )\n",
    "        t = sub.index.unique().array\n",
    "        for cur_t, prev_t in zip(t, t[1:]):\n",
    "            if cur_t is pd.NaT or prev_t is pd.NaT:\n",
    "                continue\n",
    "\n",
    "            prev = sub.loc[[prev_t], :]\n",
    "            cur = sub.loc[[cur_t], :]\n",
    "\n",
    "            # Ensure bssid is unique by averaging RSSI values\n",
    "            prev = prev.groupby('bssid', as_index=False).agg({'rssi': 'mean'})\n",
    "            cur = cur.groupby('bssid', as_index=False).agg({'rssi': 'mean'})\n",
    "\n",
    "            # Create the union of BSSIDs\n",
    "            union_bssids = np.union1d(prev['bssid'], cur['bssid'])\n",
    "\n",
    "            # Create aligned RSSI values for prev and cur based on the union of bssids\n",
    "            prev_rssi = prev.set_index('bssid').reindex(union_bssids)['rssi'].fillna(-100)\n",
    "            cur_rssi = cur.set_index('bssid').reindex(union_bssids)['rssi'].fillna(-100)\n",
    "\n",
    "            # Ensure the lengths match for cosine similarity calculation\n",
    "            if len(prev_rssi) != len(cur_rssi):\n",
    "                continue  # Skip this comparison if lengths don't match (though reindex should guarantee matching)\n",
    "\n",
    "            new_data.append(dict(\n",
    "                pcode=pcode,\n",
    "                timestamp=cur_t,\n",
    "                cosine=1 - dist.cosine(prev_rssi, cur_rssi) if len(union_bssids) > 0 else 0,\n",
    "                euclidean=1 / (1 + dist.euclidean(prev_rssi, cur_rssi)) if len(union_bssids) > 0 else 0,\n",
    "                manhattan=1 / (1 + dist.cityblock(prev_rssi, cur_rssi)) if len(union_bssids) > 0 else 0,\n",
    "                jaccard=len(np.intersect1d(prev['bssid'], cur['bssid'])) / len(union_bssids) if len(union_bssids) > 0 else 0\n",
    "            ))\n",
    "\n",
    "    new_data = pd.DataFrame(new_data).set_index(['pcode', 'timestamp'])\n",
    "    \n",
    "    return {\n",
    "        'COS': new_data['cosine'].astype('float32'),\n",
    "        'EUC': new_data['euclidean'].astype('float32'),\n",
    "        'MAN': new_data['manhattan'].astype('float32'),\n",
    "        'JAC': new_data['jaccard'].astype('float32')\n",
    "    }\n",
    "\n",
    "def _proc_installed_app(data: pd.DataFrame) -> Union[pd.Series, Dict[str, pd.Series]]:\n",
    "    new_data = []\n",
    "    \n",
    "    for pcode in data.index.get_level_values('pcode').unique():\n",
    "        sub = data.loc[(pcode, ), :].sort_index(axis=0, level='timestamp')\n",
    "        t = sub.index.unique().array\n",
    "        for cur_t, prev_t in zip(t, t.shift(1)):\n",
    "            if cur_t is pd.NaT or prev_t is pd.NaT:\n",
    "                continue\n",
    "\n",
    "            prev = sub.loc[[prev_t], :]\n",
    "            cur = sub.loc[[cur_t], :]\n",
    "            intersect = np.intersect1d(prev['packageName'], cur['packageName'])\n",
    "            union = np.union1d(prev['packageName'], cur['packageName'])\n",
    "            new_data.append(dict(\n",
    "                pcode=pcode,\n",
    "                timestamp=cur_t,\n",
    "                jaccard=len(intersect) / len(union) if len(union) > 0 else 0\n",
    "            ))\n",
    "            \n",
    "    new_data = pd.DataFrame(new_data).set_index(['pcode', 'timestamp'])\n",
    "    \n",
    "    return {\n",
    "       'JAC': new_data['jaccard'].astype('float32')\n",
    "    }\n",
    "\n",
    "\n",
    "def _proc_message_event(data: pd.DataFrame) -> Union[pd.Series, Dict[str, pd.Series]]:\n",
    "    new_data = defaultdict(list)\n",
    "    \n",
    "    for pcode in data.index.get_level_values('pcode').unique():\n",
    "        sub = data.loc[(pcode, ), :].sort_index(\n",
    "            axis=0, level='timestamp'\n",
    "        )\n",
    "\n",
    "        sent = sub.loc[\n",
    "            lambda x: x['messageBox'] == 'SENT', :\n",
    "        ].assign(\n",
    "            event=1,\n",
    "            pcode=pcode\n",
    "        ).reset_index()\n",
    "\n",
    "        recv = sub.loc[\n",
    "            lambda x: x['messageBox'] == 'INBOX', :\n",
    "        ].assign(\n",
    "            event=1,\n",
    "            pcode=pcode\n",
    "        ).reset_index()\n",
    "\n",
    "        msg = sub.assign(\n",
    "            event=1,\n",
    "            pcode=pcode\n",
    "        ).reset_index()\n",
    "\n",
    "        new_data['SNT'].append(sent)\n",
    "        new_data['RCV'].append(recv)\n",
    "        new_data['ALL'].append(msg)\n",
    "\n",
    "    return {\n",
    "        k: pd.concat(\n",
    "            v, axis=0, ignore_index=True\n",
    "        ).set_index(\n",
    "            ['pcode', 'timestamp']\n",
    "        )['event'].astype('float32') \n",
    "        for k, v in new_data.items()\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _proc_bluetooth_scan(data: pd.DataFrame) -> Union[pd.Series, Dict[str, pd.Series]]:\n",
    "    # Simply return the bondState, deviceType, and classType as they are\n",
    "    return { \n",
    "        'BondState' : data['bondState'].astype('object'),\n",
    "        'DeviceType': data['deviceType'].astype('object'), \n",
    "        'classType': data['classType'].astype('object'), \n",
    "    }\n",
    "\n",
    "def _proc_notification_event(data: pd.DataFrame) -> Union[pd.Series, Dict[str, pd.Series]]:\n",
    "    return {\n",
    "        'VIS': data['visibility'].astype('object'),\n",
    "        'CAT': data['category'].astype('object')\n",
    "    }\n",
    "\n",
    "\n",
    "def _proc_dozemode_event(data: pd.DataFrame) -> pd.Series:\n",
    "    # Ensure the data is sorted by timestamp\n",
    "    data = data.sort_values(by='timestamp')\n",
    "    return data['type'].astype('object')\n",
    "\n",
    "def _proc_fitbit_heartrate(data: pd.DataFrame) -> Union[pd.Series, Dict[str, pd.Series]]:\n",
    "    return data['value'].astype('float32')\n",
    "\n",
    "def _proc_fitbit_stepcount(data: pd.DataFrame) -> Union[pd.Series, Dict[str, pd.Series]]:\n",
    "    return data['value'].astype('float32')\n",
    "\n",
    "def _proc_fitbit_calorie(data: pd.DataFrame) -> Union[pd.Series, Dict[str, pd.Series]]:\n",
    "    return data['value'].astype('float32')\n",
    "\n",
    "def _proc_fitbit_distance(data: pd.DataFrame) -> Union[pd.Series, Dict[str, pd.Series]]:\n",
    "    return data['value'].astype('float32')\n",
    "\n",
    "\n",
    "def _proc_key(data: pd.DataFrame) -> Union[pd.Series, Dict[str, pd.Series]]:\n",
    "    data = data.rename(columns={'category': 'subcategory'})\n",
    "    data['category'] = [transform.get(item, item) for item in data['subcategory'].values]\n",
    "    return {'CAT': data['category'].astype('object'), \n",
    "            'DIST': data['distance'].astype('float32'),\n",
    "            'TIME': data['timeTaken'].astype('float32')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78ad5042",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import gc\n",
    "from functools import reduce\n",
    "import warnings\n",
    "from pandas.errors import PerformanceWarning\n",
    "import ray\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=PerformanceWarning)\n",
    "warnings.simplefilter(action=\"ignore\", category=RuntimeWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6fcbb556",
   "metadata": {},
   "outputs": [],
   "source": [
    "FUNC_PROC = {\n",
    "    # 'ActivityEvent': _proc_activity_event, #newly added\n",
    "    # 'ActivityTransition': _proc_activity_transition, #newly added\n",
    "    # 'WirelessStateEvent': _proc_wireless_state, #newly added\n",
    "    # 'Fitness-Calorie': _proc_calorie,\n",
    "    # 'Fitness-Activity': _proc_activity, #newly added\n",
    "    # 'Fitness-Distance': _proc_distance, #newly added\n",
    "    # 'Fitness-StepCount': _proc_step_count, #newly added\n",
    "    # 'AppUsageEvent': _proc_app_usage,\n",
    "    # 'BatteryEvent': _proc_battery,\n",
    "    # 'CallEvent': _proc_call,\n",
    "    # 'DataTraffic': _proc_data_traffic,\n",
    "    # 'InstalledApp': _proc_installed_app,\n",
    "    'Location': _proc_location,\n",
    "    # 'MessageEvent': _proc_message_event,\n",
    "    # 'WifiScan': _proc_wifi,\n",
    "    # 'ScreenEvent': _proc_screen,\n",
    "    # 'RingerModeEvent': _proc_ringer_mode,\n",
    "    # 'ChargeEvent': _proc_charge,\n",
    "    # 'PowerSaveEvent': _proc_power_save,\n",
    "    # 'OnOffEvent': _proc_on_off,\n",
    "    # 'BluetoothScan': _proc_bluetooth_scan,\n",
    "    # 'DozeModeEvent': _proc_dozemode_event,\n",
    "    # 'Fitbit-HeartRate': _proc_fitbit_heartrate,  \n",
    "    # 'Fitbit-StepCount': _proc_fitbit_stepcount, #newly added\n",
    "    # 'Fitbit-Calorie':_proc_fitbit_calorie, #newly added\n",
    "    # 'Fitbit-Distance':_proc_fitbit_distance, #newly added\n",
    "    # 'KeyEvent': _proc_key, #newly added  \n",
    "    # 'NotificationEvent': _proc_notification_event\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c9bfd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import gc\n",
    "from typing import Optional, Union, Dict\n",
    "import numpy as np\n",
    "import ray\n",
    "from collections import defaultdict\n",
    "from poi import PoiCluster \n",
    "from Funcs.Utility import _load_data\n",
    "\n",
    "\n",
    "def _process(data_type: str):\n",
    "    log(f'Begin to processing data: {data_type}')\n",
    "    \n",
    "    abbrev = DATA_TYPES[data_type]\n",
    "    data_raw = _load_data(data_type)\n",
    "    \n",
    "    # Debugging: Check if data_raw is empty\n",
    "    if data_raw.empty:\n",
    "        print(f\"No data loaded for {data_type}\")\n",
    "        log(f\"No data loaded for {data_type}\")\n",
    "        return {}\n",
    "    \n",
    "    data_proc = FUNC_PROC[data_type](data_raw)\n",
    "    result = dict()\n",
    "    \n",
    "    if type(data_proc) is dict:\n",
    "        for k, v in data_proc.items():\n",
    "            result[f'{abbrev}_{k}'] = v\n",
    "    else:\n",
    "        result[abbrev] = data_proc\n",
    "        \n",
    "    log(f'Complete processing data: {data_type}')\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "851af696",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 15:33:50,538\tINFO worker.py:1612 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_process pid=1879819)\u001b[0m [24-11-12 15:33:51] Begin to processing data: Location\n",
      "\u001b[2m\u001b[36m(_process pid=1879819)\u001b[0m Paths for Location: [('P126', '/var/nfs_share/D#4/newdata/P126/Location.csv'), ('P041', '/var/nfs_share/D#4/newdata/P041/Location.csv'), ('P008', '/var/nfs_share/D#4/newdata/P008/Location.csv'), ('P026', '/var/nfs_share/D#4/newdata/P026/Location.csv'), ('P065', '/var/nfs_share/D#4/newdata/P065/Location.csv'), ('P124', '/var/nfs_share/D#4/newdata/P124/Location.csv'), ('P116', '/var/nfs_share/D#4/newdata/P116/Location.csv'), ('P123', '/var/nfs_share/D#4/newdata/P123/Location.csv'), ('P091', '/var/nfs_share/D#4/newdata/P091/Location.csv'), ('P040', '/var/nfs_share/D#4/newdata/P040/Location.csv'), ('P038', '/var/nfs_share/D#4/newdata/P038/Location.csv'), ('P078', '/var/nfs_share/D#4/newdata/P078/Location.csv'), ('P061', '/var/nfs_share/D#4/newdata/P061/Location.csv'), ('P043', '/var/nfs_share/D#4/newdata/P043/Location.csv'), ('P075', '/var/nfs_share/D#4/newdata/P075/Location.csv'), ('P007', '/var/nfs_share/D#4/newdata/P007/Location.csv'), ('P131', '/var/nfs_share/D#4/newdata/P131/Location.csv'), ('P069', '/var/nfs_share/D#4/newdata/P069/Location.csv'), ('P024', '/var/nfs_share/D#4/newdata/P024/Location.csv'), ('P118', '/var/nfs_share/D#4/newdata/P118/Location.csv'), ('P029', '/var/nfs_share/D#4/newdata/P029/Location.csv'), ('P109', '/var/nfs_share/D#4/newdata/P109/Location.csv'), ('P133', '/var/nfs_share/D#4/newdata/P133/Location.csv'), ('P058', '/var/nfs_share/D#4/newdata/P058/Location.csv'), ('P094', '/var/nfs_share/D#4/newdata/P094/Location.csv'), ('P117', '/var/nfs_share/D#4/newdata/P117/Location.csv'), ('P013', '/var/nfs_share/D#4/newdata/P013/Location.csv'), ('P101', '/var/nfs_share/D#4/newdata/P101/Location.csv'), ('P027', '/var/nfs_share/D#4/newdata/P027/Location.csv'), ('P107', '/var/nfs_share/D#4/newdata/P107/Location.csv'), ('P066', '/var/nfs_share/D#4/newdata/P066/Location.csv'), ('P077', '/var/nfs_share/D#4/newdata/P077/Location.csv'), ('P079', '/var/nfs_share/D#4/newdata/P079/Location.csv'), ('P098', '/var/nfs_share/D#4/newdata/P098/Location.csv'), ('P127', '/var/nfs_share/D#4/newdata/P127/Location.csv'), ('P115', '/var/nfs_share/D#4/newdata/P115/Location.csv'), ('P010', '/var/nfs_share/D#4/newdata/P010/Location.csv'), ('P087', '/var/nfs_share/D#4/newdata/P087/Location.csv'), ('P009', '/var/nfs_share/D#4/newdata/P009/Location.csv'), ('P120', '/var/nfs_share/D#4/newdata/P120/Location.csv'), ('P092', '/var/nfs_share/D#4/newdata/P092/Location.csv'), ('P048', '/var/nfs_share/D#4/newdata/P048/Location.csv'), ('P056', '/var/nfs_share/D#4/newdata/P056/Location.csv'), ('P030', '/var/nfs_share/D#4/newdata/P030/Location.csv'), ('P014', '/var/nfs_share/D#4/newdata/P014/Location.csv'), ('P019', '/var/nfs_share/D#4/newdata/P019/Location.csv'), ('P072', '/var/nfs_share/D#4/newdata/P072/Location.csv'), ('P051', '/var/nfs_share/D#4/newdata/P051/Location.csv'), ('P121', '/var/nfs_share/D#4/newdata/P121/Location.csv'), ('P067', '/var/nfs_share/D#4/newdata/P067/Location.csv'), ('P119', '/var/nfs_share/D#4/newdata/P119/Location.csv'), ('P104', '/var/nfs_share/D#4/newdata/P104/Location.csv'), ('P090', '/var/nfs_share/D#4/newdata/P090/Location.csv'), ('P070', '/var/nfs_share/D#4/newdata/P070/Location.csv'), ('P055', '/var/nfs_share/D#4/newdata/P055/Location.csv'), ('P021', '/var/nfs_share/D#4/newdata/P021/Location.csv'), ('P049', '/var/nfs_share/D#4/newdata/P049/Location.csv'), ('P034', '/var/nfs_share/D#4/newdata/P034/Location.csv'), ('P003', '/var/nfs_share/D#4/newdata/P003/Location.csv'), ('P095', '/var/nfs_share/D#4/newdata/P095/Location.csv'), ('P097', '/var/nfs_share/D#4/newdata/P097/Location.csv'), ('P016', '/var/nfs_share/D#4/newdata/P016/Location.csv'), ('P105', '/var/nfs_share/D#4/newdata/P105/Location.csv'), ('P103', '/var/nfs_share/D#4/newdata/P103/Location.csv'), ('P050', '/var/nfs_share/D#4/newdata/P050/Location.csv'), ('P011', '/var/nfs_share/D#4/newdata/P011/Location.csv'), ('P035', '/var/nfs_share/D#4/newdata/P035/Location.csv'), ('P122', '/var/nfs_share/D#4/newdata/P122/Location.csv'), ('P088', '/var/nfs_share/D#4/newdata/P088/Location.csv'), ('P001', '/var/nfs_share/D#4/newdata/P001/Location.csv'), ('P110', '/var/nfs_share/D#4/newdata/P110/Location.csv'), ('P071', '/var/nfs_share/D#4/newdata/P071/Location.csv'), ('P096', '/var/nfs_share/D#4/newdata/P096/Location.csv'), ('P125', '/var/nfs_share/D#4/newdata/P125/Location.csv'), ('P015', '/var/nfs_share/D#4/newdata/P015/Location.csv'), ('P052', '/var/nfs_share/D#4/newdata/P052/Location.csv'), ('P099', '/var/nfs_share/D#4/newdata/P099/Location.csv'), ('P102', '/var/nfs_share/D#4/newdata/P102/Location.csv'), ('P135', '/var/nfs_share/D#4/newdata/P135/Location.csv'), ('P108', '/var/nfs_share/D#4/newdata/P108/Location.csv'), ('P084', '/var/nfs_share/D#4/newdata/P084/Location.csv'), ('P064', '/var/nfs_share/D#4/newdata/P064/Location.csv'), ('P047', '/var/nfs_share/D#4/newdata/P047/Location.csv'), ('P054', '/var/nfs_share/D#4/newdata/P054/Location.csv'), ('P114', '/var/nfs_share/D#4/newdata/P114/Location.csv'), ('P086', '/var/nfs_share/D#4/newdata/P086/Location.csv'), ('P028', '/var/nfs_share/D#4/newdata/P028/Location.csv'), ('P057', '/var/nfs_share/D#4/newdata/P057/Location.csv'), ('P073', '/var/nfs_share/D#4/newdata/P073/Location.csv'), ('P074', '/var/nfs_share/D#4/newdata/P074/Location.csv'), ('P081', '/var/nfs_share/D#4/newdata/P081/Location.csv'), ('P042', '/var/nfs_share/D#4/newdata/P042/Location.csv'), ('P002', '/var/nfs_share/D#4/newdata/P002/Location.csv'), ('P023', '/var/nfs_share/D#4/newdata/P023/Location.csv'), ('P044', '/var/nfs_share/D#4/newdata/P044/Location.csv'), ('P033', '/var/nfs_share/D#4/newdata/P033/Location.csv'), ('P018', '/var/nfs_share/D#4/newdata/P018/Location.csv'), ('P025', '/var/nfs_share/D#4/newdata/P025/Location.csv'), ('P059', '/var/nfs_share/D#4/newdata/P059/Location.csv'), ('P037', '/var/nfs_share/D#4/newdata/P037/Location.csv'), ('P076', '/var/nfs_share/D#4/newdata/P076/Location.csv'), ('P020', '/var/nfs_share/D#4/newdata/P020/Location.csv'), ('P045', '/var/nfs_share/D#4/newdata/P045/Location.csv'), ('P046', '/var/nfs_share/D#4/newdata/P046/Location.csv'), ('P022', '/var/nfs_share/D#4/newdata/P022/Location.csv'), ('P083', '/var/nfs_share/D#4/newdata/P083/Location.csv')]\n",
      "\u001b[2m\u001b[36m(_process pid=1879819)\u001b[0m Valid paths for Location: [('P126', '/var/nfs_share/D#4/newdata/P126/Location.csv'), ('P041', '/var/nfs_share/D#4/newdata/P041/Location.csv'), ('P008', '/var/nfs_share/D#4/newdata/P008/Location.csv'), ('P026', '/var/nfs_share/D#4/newdata/P026/Location.csv'), ('P065', '/var/nfs_share/D#4/newdata/P065/Location.csv'), ('P124', '/var/nfs_share/D#4/newdata/P124/Location.csv'), ('P116', '/var/nfs_share/D#4/newdata/P116/Location.csv'), ('P123', '/var/nfs_share/D#4/newdata/P123/Location.csv'), ('P091', '/var/nfs_share/D#4/newdata/P091/Location.csv'), ('P040', '/var/nfs_share/D#4/newdata/P040/Location.csv'), ('P038', '/var/nfs_share/D#4/newdata/P038/Location.csv'), ('P078', '/var/nfs_share/D#4/newdata/P078/Location.csv'), ('P061', '/var/nfs_share/D#4/newdata/P061/Location.csv'), ('P043', '/var/nfs_share/D#4/newdata/P043/Location.csv'), ('P075', '/var/nfs_share/D#4/newdata/P075/Location.csv'), ('P007', '/var/nfs_share/D#4/newdata/P007/Location.csv'), ('P131', '/var/nfs_share/D#4/newdata/P131/Location.csv'), ('P069', '/var/nfs_share/D#4/newdata/P069/Location.csv'), ('P024', '/var/nfs_share/D#4/newdata/P024/Location.csv'), ('P118', '/var/nfs_share/D#4/newdata/P118/Location.csv'), ('P029', '/var/nfs_share/D#4/newdata/P029/Location.csv'), ('P109', '/var/nfs_share/D#4/newdata/P109/Location.csv'), ('P133', '/var/nfs_share/D#4/newdata/P133/Location.csv'), ('P058', '/var/nfs_share/D#4/newdata/P058/Location.csv'), ('P094', '/var/nfs_share/D#4/newdata/P094/Location.csv'), ('P117', '/var/nfs_share/D#4/newdata/P117/Location.csv'), ('P013', '/var/nfs_share/D#4/newdata/P013/Location.csv'), ('P101', '/var/nfs_share/D#4/newdata/P101/Location.csv'), ('P027', '/var/nfs_share/D#4/newdata/P027/Location.csv'), ('P107', '/var/nfs_share/D#4/newdata/P107/Location.csv'), ('P066', '/var/nfs_share/D#4/newdata/P066/Location.csv'), ('P077', '/var/nfs_share/D#4/newdata/P077/Location.csv'), ('P079', '/var/nfs_share/D#4/newdata/P079/Location.csv'), ('P098', '/var/nfs_share/D#4/newdata/P098/Location.csv'), ('P127', '/var/nfs_share/D#4/newdata/P127/Location.csv'), ('P115', '/var/nfs_share/D#4/newdata/P115/Location.csv'), ('P010', '/var/nfs_share/D#4/newdata/P010/Location.csv'), ('P087', '/var/nfs_share/D#4/newdata/P087/Location.csv'), ('P009', '/var/nfs_share/D#4/newdata/P009/Location.csv'), ('P120', '/var/nfs_share/D#4/newdata/P120/Location.csv'), ('P092', '/var/nfs_share/D#4/newdata/P092/Location.csv'), ('P048', '/var/nfs_share/D#4/newdata/P048/Location.csv'), ('P056', '/var/nfs_share/D#4/newdata/P056/Location.csv'), ('P030', '/var/nfs_share/D#4/newdata/P030/Location.csv'), ('P014', '/var/nfs_share/D#4/newdata/P014/Location.csv'), ('P019', '/var/nfs_share/D#4/newdata/P019/Location.csv'), ('P072', '/var/nfs_share/D#4/newdata/P072/Location.csv'), ('P051', '/var/nfs_share/D#4/newdata/P051/Location.csv'), ('P121', '/var/nfs_share/D#4/newdata/P121/Location.csv'), ('P067', '/var/nfs_share/D#4/newdata/P067/Location.csv'), ('P119', '/var/nfs_share/D#4/newdata/P119/Location.csv'), ('P104', '/var/nfs_share/D#4/newdata/P104/Location.csv'), ('P090', '/var/nfs_share/D#4/newdata/P090/Location.csv'), ('P070', '/var/nfs_share/D#4/newdata/P070/Location.csv'), ('P055', '/var/nfs_share/D#4/newdata/P055/Location.csv'), ('P021', '/var/nfs_share/D#4/newdata/P021/Location.csv'), ('P049', '/var/nfs_share/D#4/newdata/P049/Location.csv'), ('P034', '/var/nfs_share/D#4/newdata/P034/Location.csv'), ('P003', '/var/nfs_share/D#4/newdata/P003/Location.csv'), ('P095', '/var/nfs_share/D#4/newdata/P095/Location.csv'), ('P097', '/var/nfs_share/D#4/newdata/P097/Location.csv'), ('P016', '/var/nfs_share/D#4/newdata/P016/Location.csv'), ('P105', '/var/nfs_share/D#4/newdata/P105/Location.csv'), ('P103', '/var/nfs_share/D#4/newdata/P103/Location.csv'), ('P050', '/var/nfs_share/D#4/newdata/P050/Location.csv'), ('P011', '/var/nfs_share/D#4/newdata/P011/Location.csv'), ('P035', '/var/nfs_share/D#4/newdata/P035/Location.csv'), ('P122', '/var/nfs_share/D#4/newdata/P122/Location.csv'), ('P088', '/var/nfs_share/D#4/newdata/P088/Location.csv'), ('P001', '/var/nfs_share/D#4/newdata/P001/Location.csv'), ('P110', '/var/nfs_share/D#4/newdata/P110/Location.csv'), ('P071', '/var/nfs_share/D#4/newdata/P071/Location.csv'), ('P096', '/var/nfs_share/D#4/newdata/P096/Location.csv'), ('P125', '/var/nfs_share/D#4/newdata/P125/Location.csv'), ('P015', '/var/nfs_share/D#4/newdata/P015/Location.csv'), ('P052', '/var/nfs_share/D#4/newdata/P052/Location.csv'), ('P099', '/var/nfs_share/D#4/newdata/P099/Location.csv'), ('P102', '/var/nfs_share/D#4/newdata/P102/Location.csv'), ('P135', '/var/nfs_share/D#4/newdata/P135/Location.csv'), ('P108', '/var/nfs_share/D#4/newdata/P108/Location.csv'), ('P084', '/var/nfs_share/D#4/newdata/P084/Location.csv'), ('P064', '/var/nfs_share/D#4/newdata/P064/Location.csv'), ('P047', '/var/nfs_share/D#4/newdata/P047/Location.csv'), ('P054', '/var/nfs_share/D#4/newdata/P054/Location.csv'), ('P114', '/var/nfs_share/D#4/newdata/P114/Location.csv'), ('P086', '/var/nfs_share/D#4/newdata/P086/Location.csv'), ('P028', '/var/nfs_share/D#4/newdata/P028/Location.csv'), ('P057', '/var/nfs_share/D#4/newdata/P057/Location.csv'), ('P073', '/var/nfs_share/D#4/newdata/P073/Location.csv'), ('P074', '/var/nfs_share/D#4/newdata/P074/Location.csv'), ('P081', '/var/nfs_share/D#4/newdata/P081/Location.csv'), ('P042', '/var/nfs_share/D#4/newdata/P042/Location.csv'), ('P002', '/var/nfs_share/D#4/newdata/P002/Location.csv'), ('P023', '/var/nfs_share/D#4/newdata/P023/Location.csv'), ('P044', '/var/nfs_share/D#4/newdata/P044/Location.csv'), ('P033', '/var/nfs_share/D#4/newdata/P033/Location.csv'), ('P018', '/var/nfs_share/D#4/newdata/P018/Location.csv'), ('P025', '/var/nfs_share/D#4/newdata/P025/Location.csv'), ('P059', '/var/nfs_share/D#4/newdata/P059/Location.csv'), ('P037', '/var/nfs_share/D#4/newdata/P037/Location.csv'), ('P076', '/var/nfs_share/D#4/newdata/P076/Location.csv'), ('P020', '/var/nfs_share/D#4/newdata/P020/Location.csv'), ('P045', '/var/nfs_share/D#4/newdata/P045/Location.csv'), ('P046', '/var/nfs_share/D#4/newdata/P046/Location.csv'), ('P022', '/var/nfs_share/D#4/newdata/P022/Location.csv'), ('P083', '/var/nfs_share/D#4/newdata/P083/Location.csv')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_process pid=1879819)\u001b[0m /tmp/ipykernel_1874141/1420200091.py:170: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "\u001b[2m\u001b[36m(_process pid=1879819)\u001b[0m /tmp/ipykernel_1874141/1420200091.py:170: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "\u001b[2m\u001b[36m(_process pid=1879819)\u001b[0m /tmp/ipykernel_1874141/1420200091.py:170: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "\u001b[2m\u001b[36m(_process pid=1879819)\u001b[0m /tmp/ipykernel_1874141/1420200091.py:170: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "\u001b[2m\u001b[36m(_process pid=1879819)\u001b[0m /tmp/ipykernel_1874141/1420200091.py:170: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "\u001b[2m\u001b[36m(_process pid=1879819)\u001b[0m /tmp/ipykernel_1874141/1420200091.py:170: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "\u001b[2m\u001b[36m(_process pid=1879819)\u001b[0m /tmp/ipykernel_1874141/1420200091.py:170: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "\u001b[2m\u001b[36m(_process pid=1879819)\u001b[0m /tmp/ipykernel_1874141/1420200091.py:170: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "\u001b[2m\u001b[36m(_process pid=1879819)\u001b[0m /tmp/ipykernel_1874141/1420200091.py:170: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "\u001b[2m\u001b[36m(_process pid=1879819)\u001b[0m /tmp/ipykernel_1874141/1420200091.py:170: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "\u001b[2m\u001b[36m(_process pid=1879819)\u001b[0m /tmp/ipykernel_1874141/1420200091.py:170: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "\u001b[2m\u001b[36m(_process pid=1879819)\u001b[0m /tmp/ipykernel_1874141/1420200091.py:170: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "\u001b[2m\u001b[36m(_process pid=1879819)\u001b[0m /tmp/ipykernel_1874141/1420200091.py:170: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "\u001b[2m\u001b[36m(_process pid=1879819)\u001b[0m /tmp/ipykernel_1874141/1420200091.py:170: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "\u001b[2m\u001b[36m(_process pid=1879819)\u001b[0m /tmp/ipykernel_1874141/1420200091.py:170: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "\u001b[2m\u001b[36m(_process pid=1879819)\u001b[0m /tmp/ipykernel_1874141/1420200091.py:170: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "\u001b[2m\u001b[36m(_process pid=1879819)\u001b[0m /tmp/ipykernel_1874141/1420200091.py:170: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "\u001b[2m\u001b[36m(_process pid=1879819)\u001b[0m /tmp/ipykernel_1874141/1420200091.py:170: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "\u001b[2m\u001b[36m(_process pid=1879819)\u001b[0m /tmp/ipykernel_1874141/1420200091.py:170: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "\u001b[2m\u001b[36m(_process pid=1879819)\u001b[0m /tmp/ipykernel_1874141/1420200091.py:170: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "\u001b[2m\u001b[36m(_process pid=1879819)\u001b[0m /tmp/ipykernel_1874141/1420200091.py:170: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "\u001b[2m\u001b[36m(_process pid=1879819)\u001b[0m /tmp/ipykernel_1874141/1420200091.py:170: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "\u001b[2m\u001b[36m(_process pid=1879819)\u001b[0m /tmp/ipykernel_1874141/1420200091.py:170: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "\u001b[2m\u001b[36m(_process pid=1879819)\u001b[0m /tmp/ipykernel_1874141/1420200091.py:170: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "\u001b[2m\u001b[36m(_process pid=1879819)\u001b[0m /tmp/ipykernel_1874141/1420200091.py:170: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "\u001b[2m\u001b[36m(_process pid=1879819)\u001b[0m /tmp/ipykernel_1874141/1420200091.py:170: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "\u001b[2m\u001b[36m(_process pid=1879819)\u001b[0m /tmp/ipykernel_1874141/1420200091.py:170: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "\u001b[2m\u001b[36m(_process pid=1879819)\u001b[0m /tmp/ipykernel_1874141/1420200091.py:170: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "\u001b[2m\u001b[36m(_process pid=1879819)\u001b[0m /tmp/ipykernel_1874141/1420200091.py:170: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "\u001b[2m\u001b[36m(_process pid=1879819)\u001b[0m /tmp/ipykernel_1874141/1420200091.py:170: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "\u001b[2m\u001b[36m(_process pid=1879819)\u001b[0m /tmp/ipykernel_1874141/1420200091.py:170: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "\u001b[2m\u001b[36m(_process pid=1879819)\u001b[0m /tmp/ipykernel_1874141/1420200091.py:170: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "\u001b[2m\u001b[36m(_process pid=1879819)\u001b[0m /tmp/ipykernel_1874141/1420200091.py:170: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "\u001b[2m\u001b[36m(_process pid=1879819)\u001b[0m /tmp/ipykernel_1874141/1420200091.py:170: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "\u001b[2m\u001b[36m(_process pid=1879819)\u001b[0m /tmp/ipykernel_1874141/1420200091.py:170: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "\u001b[2m\u001b[36m(_process pid=1879819)\u001b[0m /tmp/ipykernel_1874141/1420200091.py:170: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "\u001b[2m\u001b[36m(_process pid=1879819)\u001b[0m /tmp/ipykernel_1874141/1420200091.py:170: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "\u001b[2m\u001b[36m(_process pid=1879819)\u001b[0m /tmp/ipykernel_1874141/1420200091.py:170: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "\u001b[2m\u001b[36m(_process pid=1879819)\u001b[0m /tmp/ipykernel_1874141/1420200091.py:170: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "\u001b[2m\u001b[36m(_process pid=1879819)\u001b[0m /tmp/ipykernel_1874141/1420200091.py:170: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "\u001b[2m\u001b[36m(_process pid=1879819)\u001b[0m /tmp/ipykernel_1874141/1420200091.py:170: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "\u001b[2m\u001b[36m(_process pid=1879819)\u001b[0m /tmp/ipykernel_1874141/1420200091.py:170: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "\u001b[2m\u001b[36m(_process pid=1879819)\u001b[0m /tmp/ipykernel_1874141/1420200091.py:170: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "\u001b[2m\u001b[36m(_process pid=1879819)\u001b[0m /tmp/ipykernel_1874141/1420200091.py:170: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "\u001b[2m\u001b[36m(_process pid=1879819)\u001b[0m /tmp/ipykernel_1874141/1420200091.py:170: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "\u001b[2m\u001b[36m(_process pid=1879819)\u001b[0m /tmp/ipykernel_1874141/1420200091.py:170: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "\u001b[2m\u001b[36m(_process pid=1879819)\u001b[0m /tmp/ipykernel_1874141/1420200091.py:170: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "\u001b[2m\u001b[36m(_process pid=1879819)\u001b[0m /tmp/ipykernel_1874141/1420200091.py:170: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "\u001b[2m\u001b[36m(_process pid=1879819)\u001b[0m /tmp/ipykernel_1874141/1420200091.py:170: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "\u001b[2m\u001b[36m(_process pid=1879819)\u001b[0m /tmp/ipykernel_1874141/1420200091.py:170: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "\u001b[2m\u001b[36m(_process pid=1879819)\u001b[0m /tmp/ipykernel_1874141/1420200091.py:170: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "\u001b[2m\u001b[36m(_process pid=1879819)\u001b[0m /tmp/ipykernel_1874141/1420200091.py:170: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "\u001b[2m\u001b[36m(_process pid=1879819)\u001b[0m /tmp/ipykernel_1874141/1420200091.py:170: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "\u001b[2m\u001b[36m(_process pid=1879819)\u001b[0m /tmp/ipykernel_1874141/1420200091.py:170: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "\u001b[2m\u001b[36m(_process pid=1879819)\u001b[0m /tmp/ipykernel_1874141/1420200091.py:170: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "\u001b[2m\u001b[36m(_process pid=1879819)\u001b[0m /tmp/ipykernel_1874141/1420200091.py:170: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "\u001b[2m\u001b[36m(_process pid=1879819)\u001b[0m /tmp/ipykernel_1874141/1420200091.py:170: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "\u001b[2m\u001b[36m(_process pid=1879819)\u001b[0m /tmp/ipykernel_1874141/1420200091.py:170: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "\u001b[2m\u001b[36m(_process pid=1879819)\u001b[0m /tmp/ipykernel_1874141/1420200091.py:170: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "\u001b[2m\u001b[36m(_process pid=1879819)\u001b[0m /tmp/ipykernel_1874141/1420200091.py:170: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "\u001b[2m\u001b[36m(_process pid=1879819)\u001b[0m /tmp/ipykernel_1874141/1420200091.py:170: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "\u001b[2m\u001b[36m(_process pid=1879819)\u001b[0m /tmp/ipykernel_1874141/1420200091.py:170: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "\u001b[2m\u001b[36m(_process pid=1879819)\u001b[0m /tmp/ipykernel_1874141/1420200091.py:170: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "\u001b[2m\u001b[36m(_process pid=1879819)\u001b[0m /tmp/ipykernel_1874141/1420200091.py:170: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "\u001b[2m\u001b[36m(_process pid=1879819)\u001b[0m /tmp/ipykernel_1874141/1420200091.py:170: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "\u001b[2m\u001b[36m(_process pid=1879819)\u001b[0m /tmp/ipykernel_1874141/1420200091.py:170: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "\u001b[2m\u001b[36m(_process pid=1879819)\u001b[0m /tmp/ipykernel_1874141/1420200091.py:170: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "\u001b[2m\u001b[36m(_process pid=1879819)\u001b[0m /tmp/ipykernel_1874141/1420200091.py:170: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "\u001b[2m\u001b[36m(_process pid=1879819)\u001b[0m /tmp/ipykernel_1874141/1420200091.py:170: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "\u001b[2m\u001b[36m(_process pid=1879819)\u001b[0m /tmp/ipykernel_1874141/1420200091.py:170: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "\u001b[2m\u001b[36m(_process pid=1879819)\u001b[0m /tmp/ipykernel_1874141/1420200091.py:170: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "\u001b[2m\u001b[36m(_process pid=1879819)\u001b[0m /tmp/ipykernel_1874141/1420200091.py:170: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "\u001b[2m\u001b[36m(_process pid=1879819)\u001b[0m /tmp/ipykernel_1874141/1420200091.py:170: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "\u001b[2m\u001b[36m(_process pid=1879819)\u001b[0m /tmp/ipykernel_1874141/1420200091.py:170: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "\u001b[2m\u001b[36m(_process pid=1879819)\u001b[0m /tmp/ipykernel_1874141/1420200091.py:170: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "\u001b[2m\u001b[36m(_process pid=1879819)\u001b[0m /tmp/ipykernel_1874141/1420200091.py:170: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "\u001b[2m\u001b[36m(_process pid=1879819)\u001b[0m /tmp/ipykernel_1874141/1420200091.py:170: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "\u001b[2m\u001b[36m(_process pid=1879819)\u001b[0m /tmp/ipykernel_1874141/1420200091.py:170: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "\u001b[2m\u001b[36m(_process pid=1879819)\u001b[0m /tmp/ipykernel_1874141/1420200091.py:170: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "\u001b[2m\u001b[36m(_process pid=1879819)\u001b[0m /tmp/ipykernel_1874141/1420200091.py:170: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "\u001b[2m\u001b[36m(_process pid=1879819)\u001b[0m /tmp/ipykernel_1874141/1420200091.py:170: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "\u001b[2m\u001b[36m(_process pid=1879819)\u001b[0m /tmp/ipykernel_1874141/1420200091.py:170: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "\u001b[2m\u001b[36m(_process pid=1879819)\u001b[0m /tmp/ipykernel_1874141/1420200091.py:170: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "\u001b[2m\u001b[36m(_process pid=1879819)\u001b[0m /tmp/ipykernel_1874141/1420200091.py:170: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "\u001b[2m\u001b[36m(_process pid=1879819)\u001b[0m /tmp/ipykernel_1874141/1420200091.py:170: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "\u001b[2m\u001b[36m(_process pid=1879819)\u001b[0m /tmp/ipykernel_1874141/1420200091.py:170: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "\u001b[2m\u001b[36m(_process pid=1879819)\u001b[0m /tmp/ipykernel_1874141/1420200091.py:170: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "\u001b[2m\u001b[36m(_process pid=1879819)\u001b[0m /tmp/ipykernel_1874141/1420200091.py:170: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "\u001b[2m\u001b[36m(_process pid=1879819)\u001b[0m /tmp/ipykernel_1874141/1420200091.py:170: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "\u001b[2m\u001b[36m(_process pid=1879819)\u001b[0m /tmp/ipykernel_1874141/1420200091.py:170: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "\u001b[2m\u001b[36m(_process pid=1879819)\u001b[0m /tmp/ipykernel_1874141/1420200091.py:170: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "\u001b[2m\u001b[36m(_process pid=1879819)\u001b[0m /tmp/ipykernel_1874141/1420200091.py:170: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "\u001b[2m\u001b[36m(_process pid=1879819)\u001b[0m /tmp/ipykernel_1874141/1420200091.py:170: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "\u001b[2m\u001b[36m(_process pid=1879819)\u001b[0m /tmp/ipykernel_1874141/1420200091.py:170: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "\u001b[2m\u001b[36m(_process pid=1879819)\u001b[0m /tmp/ipykernel_1874141/1420200091.py:170: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "\u001b[2m\u001b[36m(_process pid=1879819)\u001b[0m /tmp/ipykernel_1874141/1420200091.py:170: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "\u001b[2m\u001b[36m(_process pid=1879819)\u001b[0m /tmp/ipykernel_1874141/1420200091.py:170: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "\u001b[2m\u001b[36m(_process pid=1879819)\u001b[0m /tmp/ipykernel_1874141/1420200091.py:170: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "\u001b[2m\u001b[36m(_process pid=1879819)\u001b[0m /tmp/ipykernel_1874141/1420200091.py:170: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "\u001b[2m\u001b[36m(_process pid=1879819)\u001b[0m /tmp/ipykernel_1874141/1420200091.py:170: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "\u001b[2m\u001b[36m(_process pid=1879819)\u001b[0m /tmp/ipykernel_1874141/1420200091.py:170: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "\u001b[2m\u001b[36m(_process pid=1879819)\u001b[0m /tmp/ipykernel_1874141/1420200091.py:170: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "\u001b[2m\u001b[36m(_process pid=1879819)\u001b[0m /tmp/ipykernel_1874141/1420200091.py:170: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "\u001b[2m\u001b[36m(_process pid=1879819)\u001b[0m /tmp/ipykernel_1874141/1420200091.py:170: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "\u001b[2m\u001b[36m(_process pid=1879819)\u001b[0m /tmp/ipykernel_1874141/1420200091.py:170: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "\u001b[2m\u001b[36m(_process pid=1879819)\u001b[0m /tmp/ipykernel_1874141/1420200091.py:170: PerformanceWarning: indexing past lexsort depth may impact performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_process pid=1879819)\u001b[0m [24-11-12 15:34:08] Complete processing data: Location\n"
     ]
    }
   ],
   "source": [
    "with on_ray():\n",
    "    jobs = []\n",
    "    \n",
    "    func = ray.remote(_process).remote\n",
    "    \n",
    "    for data_type in DATA_TYPES:\n",
    "        job = func(data_type)\n",
    "        jobs.append(job)\n",
    "\n",
    "    jobs = ray.get(jobs)\n",
    "    \n",
    "    jobs = reduce(lambda a, b: {**a, **b}, jobs)\n",
    "    dump(jobs, os.path.join(PATH_INTERMEDIATE, 'proc_loc_v2.pkl'))\n",
    "\n",
    "    del jobs\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2af6f44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loc_v2 = pd.read_pickle(os.path.join(PATH_INTERMEDIATE, 'proc_loc_v2.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d59fc526",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LOC_DST': pcode  timestamp                       \n",
       " P126   2021-12-06 15:30:30.622000+09:00      0.000000\n",
       "        2021-12-06 15:32:16.025000+09:00     75.783302\n",
       "        2021-12-06 15:34:15.170000+09:00     78.214386\n",
       "        2021-12-06 15:39:25.419000+09:00     16.788128\n",
       "        2021-12-06 15:41:02.878000+09:00     13.604548\n",
       "                                               ...    \n",
       " P083   2021-12-28 23:33:06.771000+09:00      5.711549\n",
       "        2021-12-28 23:39:04.200000+09:00      7.898235\n",
       "        2021-12-28 23:45:01.310000+09:00      5.523997\n",
       "        2021-12-28 23:45:36.581000+09:00      5.960837\n",
       "        2021-12-28 23:51:40.129000+09:00    101.850624\n",
       " Name: dist, Length: 612941, dtype: float32,\n",
       " 'LOC_CLS': pcode  timestamp                       \n",
       " P126   2021-12-06 15:30:30.622000+09:00      NONE\n",
       "        2021-12-06 15:32:16.025000+09:00      NONE\n",
       "        2021-12-06 15:34:15.170000+09:00      NONE\n",
       "        2021-12-06 15:39:25.419000+09:00      NONE\n",
       "        2021-12-06 15:41:02.878000+09:00    E0EAAC\n",
       "                                             ...  \n",
       " P083   2021-12-28 23:33:06.771000+09:00    BF7C60\n",
       "        2021-12-28 23:39:04.200000+09:00    BF7C60\n",
       "        2021-12-28 23:45:01.310000+09:00    BF7C60\n",
       "        2021-12-28 23:45:36.581000+09:00    BF7C60\n",
       "        2021-12-28 23:51:40.129000+09:00    BF7C60\n",
       " Name: cluster, Length: 612941, dtype: object}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_loc_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d38de3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loc = pd.read_pickle(os.path.join(PATH_INTERMEDIATE, 'proc_loc.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "828c1321",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_except_loc = pd.read_pickle(os.path.join(PATH_INTERMEDIATE, 'proc_except_loc.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "18f8f285",
   "metadata": {},
   "outputs": [],
   "source": [
    "dump({**data_loc, **data_loc_v2, **data_except_loc}, os.path.join(PATH_INTERMEDIATE, 'proc_updated.pkl'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sci-data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
