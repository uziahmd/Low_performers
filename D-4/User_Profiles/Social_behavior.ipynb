{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27ff7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Funcs.Utility import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, Callable, Union, Tuple, List, Optional, Iterable, Any\n",
    "from datetime import timedelta as td\n",
    "from scipy import stats\n",
    "import ray\n",
    "import warnings\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6cb327",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats as sp\n",
    "from scipy.signal import correlate\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from functools import reduce\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import logging\n",
    "from typing import Dict, List\n",
    "\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1182305",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_numeric_features(window_data: np.ndarray) -> Dict[str, float]:\n",
    "    if len(window_data) < 2 or np.isnan(window_data).all():\n",
    "        logger.debug(\"Window data is insufficient or contains all NaN values.\")\n",
    "        return {}\n",
    "    features = {}\n",
    "    try:\n",
    "        # Remove NaN values\n",
    "        window_data = window_data[~np.isnan(window_data)]\n",
    "        logger.debug(f\"Window data after removing NaNs: {window_data}\")\n",
    "        \n",
    "        if len(window_data) < 2:\n",
    "            logger.debug(\"After removing NaNs, window data is insufficient.\")\n",
    "            return {}\n",
    "        \n",
    "        features['MED'] = np.median(window_data)\n",
    "        features['MIN'] = np.min(window_data)\n",
    "        features['MAX'] = np.max(window_data)\n",
    "        features['AVG'] = np.mean(window_data)\n",
    "        features['VAR'] = np.var(window_data, ddof=1)\n",
    "        if features['VAR'] == 0:\n",
    "            features['SKW'] = 0\n",
    "            features['KUR'] = 0\n",
    "        else:\n",
    "            features['SKW'] = sp.skew(window_data, bias=False)\n",
    "            features['KUR'] = sp.kurtosis(window_data, bias=False)\n",
    "        features['ASC'] = np.sum(np.abs(np.diff(window_data)))\n",
    "        \n",
    "        # Binned Entropy (n = 10)\n",
    "        hist, _ = np.histogram(window_data, bins=10, density=False)\n",
    "        features['BEP'] = sp.entropy(hist)\n",
    "        \n",
    "        # Auto-correlation Features using scipy.signal.correlate\n",
    "        acf = correlate(window_data - np.mean(window_data), window_data - np.mean(window_data), mode='full')\n",
    "        acf = acf[len(acf)//2:]\n",
    "        if len(acf) > 1:\n",
    "            features['MAXLAG'] = np.argmax(acf[1:]) + 1  # exclude zero lag\n",
    "            features['MAXLAGVAL'] = np.max(acf[1:])\n",
    "            features['MINLAG'] = np.argmin(acf[1:]) + 1\n",
    "            features['MINLAGVAL'] = np.min(acf[1:])\n",
    "        else:\n",
    "            features['MAXLAG'] = 0\n",
    "            features['MAXLAGVAL'] = 0\n",
    "            features['MINLAG'] = 0\n",
    "            features['MINLAGVAL'] = 0\n",
    "        \n",
    "        # Linear Trend\n",
    "        slope, intercept, _, _, _ = sp.linregress(np.arange(len(window_data)), window_data)\n",
    "        features['LTS'] = slope\n",
    "        features['LTI'] = intercept\n",
    "        \n",
    "        # Time-series Complexity\n",
    "        std_dev = np.std(window_data)\n",
    "        if std_dev != 0:\n",
    "            norm_x = (window_data - np.mean(window_data)) / std_dev\n",
    "        else:\n",
    "            norm_x = np.zeros(len(window_data))\n",
    "        features['CID'] = np.sqrt(np.sum(np.diff(norm_x) ** 2))\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error extracting numeric features: {e}\")\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec99e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_nominal_features(window_data: np.ndarray, is_bounded: bool) -> Dict[str, float]:\n",
    "    if len(window_data) == 0:\n",
    "        return {}\n",
    "    features = {}\n",
    "    try:\n",
    "        unique_values, counts = np.unique(window_data, return_counts=True)\n",
    "        entropy = sp.entropy(counts)\n",
    "        features['ETP'] = entropy\n",
    "        features['ASC'] = np.sum(window_data[1:] != window_data[:-1])\n",
    "        \n",
    "        if is_bounded:\n",
    "            support_features = {f'SUP:{val}': count for val, count in zip(unique_values, counts)}\n",
    "            features.update(support_features)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error extracting nominal features: {e}\")\n",
    "        \n",
    "    return features\n",
    "\n",
    "def impute_support_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    support_features = df.columns[df.columns.str.startswith('SUP:')]\n",
    "    df[support_features] = df[support_features].fillna(0)\n",
    "    return df\n",
    "\n",
    "def sliding_window_feature_extraction(\n",
    "    combined_df: pd.DataFrame,\n",
    "    window_size_minutes: int,\n",
    "    step_size_minutes: int = None,\n",
    "    selected_features: List[str] = None\n",
    ") -> pd.DataFrame:\n",
    "    if step_size_minutes is None:\n",
    "        step_size_minutes = window_size_minutes  # Default to non-overlapping windows\n",
    "    \n",
    "    extracted_features = []\n",
    "    \n",
    "    # Ensure the DataFrame is sorted by pcode and timestamp\n",
    "    combined_df = combined_df.sort_index(level=['pcode', 'timestamp'])\n",
    "    \n",
    "    for pcode, group in combined_df.groupby(level='pcode'):\n",
    "        # Reset 'pcode' level and keep 'timestamp' as a column\n",
    "        group = group.reset_index(level='pcode', drop=True).reset_index()\n",
    "        \n",
    "        # Set 'timestamp' as datetime index\n",
    "        group = group.set_index('timestamp')\n",
    "        \n",
    "        # Define the sliding window\n",
    "        start_time = group.index.min()\n",
    "        end_time = group.index.max()\n",
    "        \n",
    "        current_start = start_time\n",
    "        current_end = current_start + pd.Timedelta(minutes=window_size_minutes)\n",
    "        \n",
    "        while current_end <= end_time:\n",
    "            window = group[current_start:current_end]\n",
    "            feature_row = {}\n",
    "            feature_row['pcode'] = pcode\n",
    "            feature_row['window_end_time'] = current_end\n",
    "            \n",
    "            for feature in window.columns:\n",
    "                if selected_features and feature not in selected_features:\n",
    "                    continue  # Skip features not in the selected list\n",
    "                \n",
    "                data = window[feature].dropna().values\n",
    "                if len(data) < 2:\n",
    "                    # Skip feature extraction if insufficient data points\n",
    "                    continue\n",
    "\n",
    "                # Revise feature extraction for MSG and CALL data\n",
    "                if feature in ['MSG_SNT', 'MSG_RCV', 'MSG_ALL', 'CALL_DUR']:\n",
    "\n",
    "                    if feature.startswith('MSG'):\n",
    "                        # Sum up MSG features, impute NaN with 0\n",
    "                        feature_row[\"MSG#CNT\"] = np.nansum(data)  # np.nansum() will ignore NaNs and sum up the values\n",
    "                    elif feature == 'CALL_DUR':\n",
    "                        # Count non-NaN values greater than 0 and sum them up\n",
    "                        valid_data = data[data > 0]\n",
    "                        feature_row[\"CALL#CNT\"] = np.sum(~np.isnan(valid_data))  # Count non-NaN values greater than 0\n",
    "                        feature_row[\"CALL#DUR\"] = np.nansum(valid_data)  # Sum the values greater than 0 ignoring NaNs\n",
    "\n",
    "                else:\n",
    "                    # Nominal features (if any)\n",
    "                    # Assuming nominal features are non-numeric\n",
    "                    nominal_feats = extract_nominal_features(data, is_bounded=True)\n",
    "                    if not nominal_feats:\n",
    "                        continue  # Skip if no features extracted\n",
    "                    nominal_feats = {f\"{feature}#{k}\": v for k, v in nominal_feats.items()}\n",
    "                    feature_row.update(nominal_feats)\n",
    "            \n",
    "            # Only add the feature row if it contains at least one feature\n",
    "            if feature_row:\n",
    "                extracted_features.append(feature_row)\n",
    "            \n",
    "            # Move the window\n",
    "            current_start += pd.Timedelta(minutes=step_size_minutes)\n",
    "            current_end = current_start + pd.Timedelta(minutes=window_size_minutes)\n",
    "    \n",
    "    feature_df = pd.DataFrame(extracted_features)\n",
    "    if not feature_df.empty:\n",
    "        feature_df = feature_df.set_index(['pcode', 'window_end_time'])\n",
    "    \n",
    "        # Impute support features\n",
    "        feature_df = impute_support_features(feature_df)\n",
    "    \n",
    "    return feature_df\n",
    "\n",
    "def normalize_features(feature_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    scaler = StandardScaler()\n",
    "    scaled_data = scaler.fit_transform(feature_df)\n",
    "    scaled_df = pd.DataFrame(scaled_data, index=feature_df.index, columns=feature_df.columns)\n",
    "    return scaled_df\n",
    "\n",
    "def find_optimal_clusters(scaled_df: pd.DataFrame, max_k: int = 10) -> int:\n",
    "    best_k = 2\n",
    "    best_score = -1\n",
    "    for k in range(2, max_k + 1):\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "        labels = kmeans.fit_predict(scaled_df)\n",
    "        score = silhouette_score(scaled_df, labels)\n",
    "        logger.info(f\"K={k}, Silhouette Score={score}\")\n",
    "        if score > best_score:\n",
    "            best_k = k\n",
    "            best_score = score\n",
    "    logger.info(f\"Optimal number of clusters: {best_k} with Silhouette Score: {best_score}\")\n",
    "    return best_k\n",
    "\n",
    "def perform_kmeans_clustering(scaled_df: pd.DataFrame, n_clusters: int) -> pd.Series:\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    cluster_labels = kmeans.fit_predict(scaled_df)\n",
    "    return pd.Series(cluster_labels, index=scaled_df.index, name='Cluster')\n",
    "\n",
    "def visualize_clusters_pca(scaled_df: pd.DataFrame, cluster_labels: pd.Series) -> None:\n",
    "    pca = PCA(n_components=2, random_state=42)\n",
    "    principal_components = pca.fit_transform(scaled_df)\n",
    "    pca_df = pd.DataFrame(data=principal_components, columns=['PC1', 'PC2'], index=scaled_df.index)\n",
    "    pca_df['Cluster'] = cluster_labels\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.scatterplot(x='PC1', y='PC2', hue='Cluster', data=pca_df, palette='viridis', s=100, alpha=0.7)\n",
    "    plt.title('PCA Visualization of User Clusters Based on Social Behavior')\n",
    "    plt.xlabel('Principal Component 1')\n",
    "    plt.ylabel('Principal Component 2')\n",
    "    plt.legend(title='Cluster')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d46fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle(os.path.join(PATH_INTERMEDIATE, 'proc_except_loc.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ddf315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the type of the loaded data\n",
    "print(f\"Type of data: {type(data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c13afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all keys in the dictionary\n",
    "print(\"Keys in the data dictionary:\")\n",
    "print(data.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815a9a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5922cd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect 'MSG_SNT'\n",
    "msg_snt = data.get('MSG_SNT')\n",
    "print(f\"Type of 'MSG_SNT': {type(msg_snt)}\")\n",
    "print(msg_snt.head())\n",
    "\n",
    "# Inspect 'CALL_DUR'\n",
    "call_dur = data.get('CALL_DUR')\n",
    "print(f\"Type of 'CALL_DUR': {type(call_dur)}\")\n",
    "print(call_dur.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51374f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the list of relevant keys for Social Behavior\n",
    "relevant_keys = ['MSG_SNT', 'MSG_RCV', 'MSG_ALL', 'CALL_DUR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b44b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define the list of relevant keys for Social Behavior\n",
    "relevant_keys = ['MSG_SNT', 'MSG_RCV', 'MSG_ALL', 'CALL_DUR']\n",
    "\n",
    "# Initialize an empty list to collect DataFrames\n",
    "df_list = []\n",
    "skipped_keys = []\n",
    "\n",
    "# Function to remove duplicates by keeping the first occurrence\n",
    "def remove_duplicates(series, key):\n",
    "    initial_count = len(series)\n",
    "    series_unique = series[~series.index.duplicated(keep='first')]\n",
    "    duplicates_removed = initial_count - len(series_unique)\n",
    "    if duplicates_removed > 0:\n",
    "        print(f\"Removed {duplicates_removed} duplicate entries in {key}.\")\n",
    "    else:\n",
    "        print(f\"No duplicates found in {key}.\")\n",
    "    return series_unique\n",
    "\n",
    "# Iterate through only the relevant keys\n",
    "for key in tqdm(relevant_keys, desc=\"Processing relevant items\", unit=\"item\"):\n",
    "    if key not in data:\n",
    "        print(f\"Key '{key}' not found in data. Skipping.\")\n",
    "        skipped_keys.append((key, 'Not Found'))\n",
    "        continue\n",
    "    \n",
    "    value = data[key]\n",
    "    print(f\"\\nProcessing key: {key}, Type: {type(value)}\")\n",
    "    \n",
    "    try:\n",
    "        if isinstance(value, pd.Series):\n",
    "            # Remove duplicates by keeping the first occurrence\n",
    "            value_unique = remove_duplicates(value, key)\n",
    "            # Convert Series to DataFrame\n",
    "            df = value_unique.to_frame(name=key)\n",
    "            df_list.append(df)\n",
    "        elif isinstance(value, pd.DataFrame):\n",
    "            # Prefix column names to avoid conflicts\n",
    "            df = value.add_prefix(f\"{key}_\")\n",
    "            # Remove duplicates if necessary\n",
    "            # Assuming the DataFrame has a MultiIndex ('pcode', 'timestamp')\n",
    "            if df.index.duplicated().any():\n",
    "                print(f\"Removing duplicates in DataFrame {key} by keeping the first occurrence.\")\n",
    "                df = df[~df.index.duplicated(keep='first')]\n",
    "            df_list.append(df)\n",
    "        else:\n",
    "            print(f\"Skipping key: {key}, Type: {type(value)}\")\n",
    "            skipped_keys.append((key, type(value)))\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing key: {key}. Error: {e}\")\n",
    "        skipped_keys.append((key, type(value)))\n",
    "\n",
    "# After the loop, report any skipped keys\n",
    "if skipped_keys:\n",
    "    print(\"\\nSkipped the following keys:\")\n",
    "    for key, dtype in skipped_keys:\n",
    "        print(f\" - {key}: {dtype}\")\n",
    "\n",
    "# Proceed only if there are DataFrames to concatenate\n",
    "if df_list:\n",
    "    try:\n",
    "        # Concatenate all DataFrames on the index with outer join to include all combinations\n",
    "        combined_df = pd.concat(df_list, axis=1, join='outer')\n",
    "        print(\"\\nSuccessfully combined relevant DataFrames.\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError concatenating DataFrames: {e}\")\n",
    "        combined_df = pd.DataFrame()  # Create an empty DataFrame to prevent further errors\n",
    "else:\n",
    "    print(\"\\nNo relevant data found to process. Exiting.\")\n",
    "    combined_df = pd.DataFrame()\n",
    "\n",
    "# Display the first few rows of the combined DataFrame\n",
    "if not combined_df.empty:\n",
    "    print(\"\\nCombined DataFrame Head:\")\n",
    "    print(combined_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf03eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not combined_df.empty:\n",
    "    # Check for missing values\n",
    "    missing_values = combined_df.isnull().sum()\n",
    "    print(\"\\nMissing values per column:\")\n",
    "    print(missing_values[missing_values > 0])\n",
    "    \n",
    "    # Handle missing values\n",
    "    for col in combined_df.columns:\n",
    "        if pd.api.types.is_numeric_dtype(combined_df[col]):\n",
    "            combined_df[col].fillna(0, inplace=True)\n",
    "        else:\n",
    "            combined_df[col].fillna('unknown', inplace=True)\n",
    "    \n",
    "    # Verify all missing values are handled\n",
    "    total_missing = combined_df.isnull().sum().sum()\n",
    "    print(f\"\\nTotal missing values after handling: {total_missing}\")\n",
    "    \n",
    "    # Ensure the index is a MultiIndex with 'pcode' and 'timestamp'\n",
    "    if not isinstance(combined_df.index, pd.MultiIndex):\n",
    "        combined_df.reset_index(inplace=True)\n",
    "        combined_df.set_index(['pcode', 'timestamp'], inplace=True)\n",
    "        print(\"\\nMultiIndex has been set to ['pcode', 'timestamp'].\")\n",
    "    else:\n",
    "        print(\"\\nMultiIndex is already set.\")\n",
    "    \n",
    "    # Convert 'timestamp' to datetime if not already\n",
    "    if not pd.api.types.is_datetime64_any_dtype(combined_df.index.get_level_values('timestamp')):\n",
    "        combined_df.index = combined_df.index.set_levels(\n",
    "            [combined_df.index.get_level_values('pcode'),\n",
    "             pd.to_datetime(combined_df.index.get_level_values('timestamp'))]\n",
    "        )\n",
    "        print(\"Timestamps have been converted to datetime.\")\n",
    "    \n",
    "    # Display the first few rows after cleaning\n",
    "    print(\"\\nCleaned Combined DataFrame Head:\")\n",
    "    print(combined_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210da24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = ['MSG_SNT', 'MSG_RCV', 'MSG_ALL', 'CALL_DUR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0d3416",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_social = combined_df[selected_features]\n",
    "\n",
    "# Display the first few rows\n",
    "print(\"Labels Social Head:\")\n",
    "print(labels_social.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40ec500",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f748e7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop non-unique multi-index rows and keep the first occurrence\n",
    "combined_df = combined_df[~combined_df.index.duplicated(keep='first')]\n",
    "\n",
    "# # Reset the index to a default integer index, making `pcode` and `timestamp` columns instead of indices\n",
    "# combined_df = combined_df.reset_index()\n",
    "# combined_df = combined_df.fillna(method='ffill').fillna(method='bfill')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd66e6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nCombined DataFrame after removing duplicates:\")\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb4ceaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 1c: Re-establish MultiIndex with 'pcode' and 'timestamp'\n",
    "# combined_df = combined_df.set_index(['pcode', 'timestamp'])\n",
    "\n",
    "# Display the DataFrame with the new MultiIndex and external irrelevant index\n",
    "print(\"\\nCombined DataFrame with MultiIndex and External Irrelevant Index:\")\n",
    "print(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a64bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nIndex Names:\", combined_df.index.names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f224df06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "window_size = 60 # minutes\n",
    "step_size = 60    # minutes (non-overlapping)\n",
    "selected_features = ['MSG_SNT', 'MSG_RCV', 'MSG_ALL', 'CALL_DUR']\n",
    "\n",
    "features_df = sliding_window_feature_extraction(\n",
    "    combined_df=combined_df,\n",
    "    window_size_minutes=window_size,\n",
    "    step_size_minutes=step_size,\n",
    "    selected_features=selected_features\n",
    ")\n",
    "\n",
    "logger.info(\"\\nExtracted Features DataFrame:\")\n",
    "print(features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12106c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf07fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_counts = features_df.isna().sum()\n",
    "nan_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe56d81e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "non_nan_rows = features_df.dropna()\n",
    "print(non_nan_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1598dd0d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Check for NaN values\n",
    "nan_counts = features_df.isna().sum()\n",
    "print(\"Missing Values per Feature:\\n\", nan_counts)\n",
    "\n",
    "# If 'features_df' has a significant number of NaNs, handle them\n",
    "# Here, we'll perform mean imputation for simplicity\n",
    "\n",
    "imputer = SimpleImputer(strategy='constant', fill_value=0)\n",
    "features_df_imputed = pd.DataFrame(\n",
    "    imputer.fit_transform(features_df),\n",
    "    index=features_df.index,\n",
    "    columns=features_df.columns\n",
    ")\n",
    "\n",
    "# Verify that there are no NaNs left\n",
    "nan_counts_imputed = features_df_imputed.isna().sum()\n",
    "print(\"\\nMissing Values After Imputation:\\n\", nan_counts_imputed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9cd4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e98c4dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Reset index to access 'pcode' as a column\n",
    "features_df_imputed_reset = features_df_imputed.reset_index()\n",
    "\n",
    "# Define aggregation functions\n",
    "# For each feature, compute mean and standard deviation\n",
    "aggregation_functions = {col: ['mean', 'std', 'sum'] for col in features_df_imputed.columns}\n",
    "\n",
    "# Aggregate features per user\n",
    "user_profiles = features_df_imputed_reset.groupby('pcode').agg(aggregation_functions)\n",
    "\n",
    "# Flatten MultiIndex columns\n",
    "user_profiles.columns = ['_'.join(col).strip() for col in user_profiles.columns.values]\n",
    "\n",
    "# Display the aggregated user profiles\n",
    "print(\"\\nAggregated User Profiles:\\n\", user_profiles.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c998ca8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8d4aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Ensure reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "# Get list of unique users\n",
    "unique_users = user_profiles.index.tolist()\n",
    "\n",
    "# # Randomly select 5 users\n",
    "# if len(unique_users) < 5:\n",
    "#     selected_users = unique_users\n",
    "#     print(\"Less than 5 users available. Selecting all users.\")\n",
    "# else:\n",
    "#     selected_users = random.sample(unique_users, 5)\n",
    "# print(\"\\nSelected Users for Comparison:\", selected_users)\n",
    "\n",
    "selected_users = ['P102', 'P020', 'P007', 'P120', 'P045']\n",
    "\n",
    "# Extract profiles of selected users\n",
    "selected_user_profiles = user_profiles.loc[selected_users]\n",
    "print(\"\\nSelected User Profiles:\\n\", selected_user_profiles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be35f135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a subset of features to visualize\n",
    "features_to_plot = [\n",
    "        \"MSG#CNT_mean\",   \n",
    "        \"MSG#CNT_std\",  \n",
    "        \"CALL#CNT_mean\",\n",
    "        \"CALL#CNT_std\",\n",
    "        \"CALL#DUR_mean\"    \n",
    "        \"CALL#DUR_std\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c6b647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.manifold import TSNE\n",
    "\n",
    "# # Prepare data for t-SNE\n",
    "# X = selected_user_profiles.values\n",
    "\n",
    "# # Initialize t-SNE\n",
    "# tsne = TSNE(n_components=2, random_state=42, perplexity=3, n_iter=1000)\n",
    "\n",
    "# # Fit and transform the data\n",
    "# X_tsne = tsne.fit_transform(X)\n",
    "\n",
    "# # Create a DataFrame for plotting\n",
    "# tsne_df = pd.DataFrame({\n",
    "#     'TSNE1': X_tsne[:, 0],\n",
    "#     'TSNE2': X_tsne[:, 1],\n",
    "#     'User': selected_user_profiles.index\n",
    "# })\n",
    "\n",
    "# # Plot t-SNE results\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# sns.scatterplot(\n",
    "#     x='TSNE1', y='TSNE2',\n",
    "#     hue='User',\n",
    "#     palette='viridis',\n",
    "#     data=tsne_df,\n",
    "#     s=100,\n",
    "#     alpha=0.8\n",
    "# )\n",
    "# plt.title('t-SNE Visualization of Selected User Profiles')\n",
    "# plt.xlabel('t-SNE Component 1')\n",
    "# plt.ylabel('t-SNE Component 2')\n",
    "# plt.legend(title='User')\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "import pandas as pd\n",
    "\n",
    "# Prepare data for t-SNE\n",
    "X = selected_user_profiles.values\n",
    "\n",
    "# Initialize t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=3, n_iter=1000)\n",
    "\n",
    "# Fit and transform the data\n",
    "X_tsne = tsne.fit_transform(X)\n",
    "\n",
    "# Create a DataFrame for plotting\n",
    "tsne_df = pd.DataFrame({\n",
    "    'TSNE1': X_tsne[:, 0],\n",
    "    'TSNE2': X_tsne[:, 1],\n",
    "    'User': selected_user_profiles.index\n",
    "})\n",
    "\n",
    "# Plot t-SNE results\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(\n",
    "    x='TSNE1', y='TSNE2',\n",
    "    hue='User',\n",
    "    palette='tab10',  # Changed palette from 'viridis' to 'coolwarm'\n",
    "    data=tsne_df,\n",
    "    s=100,\n",
    "    alpha=0.8\n",
    ")\n",
    "plt.title('t-SNE Visualization of Selected User Profiles')\n",
    "plt.xlabel('t-SNE Component 1')\n",
    "plt.ylabel('t-SNE Component 2')\n",
    "plt.legend(title='User')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143adcc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_user_profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed8536f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Initialize PCA\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "\n",
    "# Fit and transform the data\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Create a DataFrame for plotting\n",
    "pca_df = pd.DataFrame({\n",
    "    'PCA1': X_pca[:, 0],\n",
    "    'PCA2': X_pca[:, 1],\n",
    "    'User': selected_user_profiles.index\n",
    "})\n",
    "\n",
    "# Plot PCA results\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(\n",
    "    x='PCA1', y='PCA2',\n",
    "    hue='User',\n",
    "    palette='tab10',\n",
    "    data=pca_df,\n",
    "    s=100,\n",
    "    alpha=0.8\n",
    ")\n",
    "plt.title('PCA Visualization of Selected User Profiles')\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.legend(title='User')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb44fbcd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Select features to plot (remove duplicates)\n",
    "features_to_plot = [\n",
    "    \"MSG#CNT_mean\",   \n",
    "    \"MSG#CNT_std\",  \n",
    "    \"CALL#CNT_mean\",\n",
    "    \"CALL#CNT_std\",\n",
    "    \"CALL#DUR_mean\",   \n",
    "    \"CALL#DUR_std\",\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745cf9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# # Define features to plot (removing duplicates)\n",
    "# features_to_plot = [\n",
    "#     \"MSG#CNT_mean\",   \n",
    "#     \"MSG#CNT_std\",  \n",
    "#     \"MSG#CNT_sum\",  \n",
    "#     \"CALL#CNT_mean\",\n",
    "#     \"CALL#CNT_std\",\n",
    "#     \"CALL#CNT_sum\",\n",
    "#     \"CALL#DUR_mean\",   \n",
    "#     \"CALL#DUR_std\",\n",
    "#     \"CALL#DUR_sum\"\n",
    "# ]\n",
    "\n",
    "# # Create a copy of the selected user profiles DataFrame with only features to plot\n",
    "# features_df = selected_user_profiles[features_to_plot]\n",
    "\n",
    "# # Plot each feature in its own subplot\n",
    "# num_features = len(features_to_plot)\n",
    "# fig, axes = plt.subplots(num_features, 1, figsize=(10, num_features * 4), sharex=False)\n",
    "\n",
    "# for i, feature in enumerate(features_to_plot):\n",
    "#     sns.barplot(\n",
    "#         x=selected_user_profiles.index,\n",
    "#         y=feature,\n",
    "#         data=features_df,\n",
    "#         palette='viridis',\n",
    "#         ax=axes[i]\n",
    "#     )\n",
    "#     axes[i].set_title(f'Feature: {feature}')\n",
    "#     axes[i].set_xlabel('User')\n",
    "#     axes[i].set_ylabel('Value')\n",
    "#     axes[i].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Define features to plot (removing duplicates)\n",
    "features_to_plot = [\n",
    "    \"MSG#CNT_mean\",\n",
    "    \"MSG#CNT_std\",\n",
    "    \"CALL#CNT_mean\",\n",
    "    \"CALL#CNT_std\",\n",
    "    \"CALL#DUR_mean\",\n",
    "    \"CALL#DUR_std\"\n",
    "]\n",
    "\n",
    "# Create a copy of the selected user profiles DataFrame with only features to plot\n",
    "features_df = selected_user_profiles[features_to_plot]\n",
    "\n",
    "# Create a figure with 6 subplots arranged in a 3x2 format\n",
    "fig, axes = plt.subplots(3, 2, figsize=(18, 15))\n",
    "\n",
    "# Plot each feature in a separate subplot\n",
    "for i, feature in enumerate(features_to_plot):\n",
    "    row, col = divmod(i, 2)\n",
    "    sns.barplot(x='pcode', y=feature, data=selected_user_profiles.reset_index(), ax=axes[row, col], palette='tab10')\n",
    "    axes[row, col].set_title(feature)\n",
    "    axes[row, col].set_xlabel('User')\n",
    "    axes[row, col].set_ylabel('Value')\n",
    "    axes[row, col].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Adjust layout to prevent overlapping\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf15963",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "# Define features to plot (ensure the missing comma is added)\n",
    "features_to_plot = [\n",
    "    \"MSG#CNT_mean\",\n",
    "    \"MSG#CNT_std\",\n",
    "    \"CALL#CNT_mean\",\n",
    "    \"CALL#CNT_std\",\n",
    "    \"CALL#DUR_mean\",    # Added comma\n",
    "    \"CALL#DUR_std\"\n",
    "]\n",
    "\n",
    "# Verify that all features exist in the selected_user_profiles\n",
    "missing_features = [feature for feature in features_to_plot if feature not in selected_user_profiles.columns]\n",
    "if missing_features:\n",
    "    logger.warning(f\"The following features are missing in the selected_user_profiles and will be skipped: {missing_features}\")\n",
    "    features_to_plot = [feature for feature in features_to_plot if feature in selected_user_profiles.columns]\n",
    "\n",
    "# Extract the data for the radar chart\n",
    "radar_data = selected_user_profiles[features_to_plot]\n",
    "\n",
    "# Normalize the data for each feature to [0, 1] for better visualization\n",
    "normalized_data = (radar_data - radar_data.min()) / (radar_data.max() - radar_data.min())\n",
    "\n",
    "# Number of variables\n",
    "num_vars = len(features_to_plot)\n",
    "\n",
    "# Split the feature names to make them more readable (optional)\n",
    "# For example, replace '#' with ' ' and '_' with ' '\n",
    "feature_labels = [feature.replace('#', ' ').replace('_', ' ') for feature in features_to_plot]\n",
    "\n",
    "# Calculate angle for each axis in the radar chart\n",
    "angles = np.linspace(0, 2 * np.pi, num_vars, endpoint=False).tolist()\n",
    "angles += angles[:1]  # Complete the loop\n",
    "\n",
    "# Initialize the radar plot\n",
    "fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(polar=True))\n",
    "\n",
    "# Set the theta offset and direction\n",
    "ax.set_theta_offset(np.pi / 2)\n",
    "ax.set_theta_direction(-1)\n",
    "\n",
    "# Draw one axis per variable + add labels\n",
    "plt.xticks(angles[:-1], feature_labels, color='grey', size=12)\n",
    "\n",
    "# Draw ylabels\n",
    "ax.set_rlabel_position(0)\n",
    "plt.yticks([0.2, 0.4, 0.6, 0.8], [\"0.2\", \"0.4\", \"0.6\", \"0.8\"], color=\"grey\", size=10)\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "# Plot each user\n",
    "for idx, (user, row) in enumerate(normalized_data.iterrows()):\n",
    "    values = row.tolist()\n",
    "    values += values[:1]  # Complete the loop\n",
    "    ax.plot(angles, values, linewidth=2, linestyle='solid', label=user)\n",
    "    ax.fill(angles, values, alpha=0.25)\n",
    "\n",
    "# Add a title\n",
    "plt.title('Radar Chart of Selected User Profiles', size=20, y=1.05)\n",
    "\n",
    "# Add a legend\n",
    "plt.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9efb24",
   "metadata": {},
   "source": [
    "# COHEN-D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6828e9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7563dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of target pcode values\n",
    "target_pcodes = ['P102', 'P020', 'P007', 'P120', 'P045']\n",
    "\n",
    "# Filter the DataFrame for the specified pcode values\n",
    "selected_users_df = features_df_imputed.loc[target_pcodes].reset_index()\n",
    "\n",
    "# Display the new DataFrame\n",
    "print(selected_users_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9850949f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "# Assuming selected_users_df is already defined\n",
    "\n",
    "# Step 1: Normalize each column using Min-Max normalization\n",
    "normalized_df = selected_users_df.copy()\n",
    "columns_to_normalize = [\n",
    "    \"MSG#CNT\",\n",
    "    \"CALL#CNT\",\n",
    "    \"CALL#DUR\"\n",
    "]\n",
    "for column in columns_to_normalize:\n",
    "    min_val = selected_users_df[column].min()\n",
    "    max_val = selected_users_df[column].max()\n",
    "    normalized_df[column] = (selected_users_df[column] - min_val) / (max_val - min_val)\n",
    "\n",
    "# Step 2: Group by 'pcode' and calculate mean and standard deviation for the normalized columns\n",
    "grouped_mean = normalized_df.groupby('pcode').mean()[columns_to_normalize]\n",
    "grouped_std = normalized_df.groupby('pcode').std()[columns_to_normalize]\n",
    "\n",
    "# Step 3: Clip the lower bound of the error bars so they do not extend below zero\n",
    "lower_bounds = grouped_mean - grouped_std\n",
    "lower_bounds[lower_bounds < 0] = 0\n",
    "adjusted_std = grouped_mean - lower_bounds\n",
    "\n",
    "# Step 4: Define the plot size\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "# Step 5: Generate a large list of distinguishable colors\n",
    "colors = itertools.cycle([\n",
    "    'red', 'blue', 'green', 'orange', 'purple', 'brown', 'pink', 'gray', 'olive', 'cyan', \n",
    "    'navy', 'magenta', 'gold', 'lime', 'teal'\n",
    "])\n",
    "\n",
    "# Step 6: Plot mean values with improved error bars for each column, using distinct colors\n",
    "bar_width = 0.8 / len(columns_to_normalize)  # Adjust bar width to fit all columns\n",
    "x = np.arange(len(grouped_mean.index))  # x positions for the groups\n",
    "\n",
    "for idx, column in enumerate(columns_to_normalize):\n",
    "    color = next(colors)  # Get the next color from the cycle\n",
    "    # Plot the mean bars\n",
    "    ax.bar(x + idx * bar_width, grouped_mean[column], width=bar_width, label=column, color=color, alpha=0.8)\n",
    "    # Add error bars with a dotted line style\n",
    "    ax.errorbar(x + idx * bar_width, grouped_mean[column], yerr=adjusted_std[column], fmt='none',\n",
    "                ecolor='black', elinewidth=1, linestyle=':', capsize=2, alpha=0.5)\n",
    "\n",
    "\n",
    "# Step 7: Set labels, title, and legend (legend positioned outside the figure)\n",
    "ax.set_xlabel(\"Pcode\")\n",
    "ax.set_ylabel(\"Normalized Values\")\n",
    "ax.set_title(\"Mean/Std of Normalized Mobility Features Across Users (normalized by feature)\")\n",
    "\n",
    "# Configure y-axis\n",
    "ax.set_ylim(0, 0.06)  # Set y-axis range from 0 to 1 for normalized values\n",
    "\n",
    "\n",
    "# Set xticks at the center of grouped bars\n",
    "ax.set_xticks(x + bar_width * (len(columns_to_normalize) / 2))\n",
    "ax.set_xticklabels(grouped_mean.index, rotation=45)\n",
    "\n",
    "# Place legend outside the plot\n",
    "ax.legend(loc='upper left', bbox_to_anchor=(1.05, 1), borderaxespad=0.)\n",
    "\n",
    "# Step 8: Adjust layout and show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8dbfbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming selected_users_df is already defined and 'PCode' is the user identifier\n",
    "\n",
    "# Define the columns to be used for calculating Cohen's D\n",
    "selected_columns = [\n",
    "    \"MSG#CNT\",\n",
    "    \"CALL#CNT\",\n",
    "    \"CALL#DUR\"\n",
    "]\n",
    "\n",
    "# Step 1: Create a function to calculate absolute Cohen's D\n",
    "def cohen_d(x1, x2):\n",
    "    x1 = x1.dropna()\n",
    "    x2 = x2.dropna()\n",
    "    n1, n2 = len(x1), len(x2)\n",
    "    if n1 < 2 or n2 < 2:\n",
    "        return np.nan\n",
    "    mean1, mean2 = x1.mean(), x2.mean()\n",
    "    var1, var2 = x1.var(ddof=1), x2.var(ddof=1)\n",
    "    pooled_std = np.sqrt(\n",
    "        ((n1 - 1) * var1 + (n2 - 1) * var2) / (n1 + n2 - 2)\n",
    "    )\n",
    "    if pooled_std == 0:\n",
    "        return np.nan\n",
    "    else:\n",
    "        return abs((mean1 - mean2) / pooled_std)\n",
    "\n",
    "# Step 2: Extract unique user codes and initialize a matrix for mean absolute Cohen's D values\n",
    "user_codes = selected_users_df['pcode'].unique()  # Updated column name\n",
    "cohen_d_matrix = pd.DataFrame(index=user_codes, columns=user_codes)\n",
    "\n",
    "# Step 3: Calculate mean absolute Cohen's D for each pair of users across selected columns\n",
    "for user1, user2 in itertools.product(user_codes, repeat=2):\n",
    "    if user1 == user2:\n",
    "        cohen_d_matrix.loc[user1, user2] = 0  # Diagonal set to zero\n",
    "        continue\n",
    "    data1 = selected_users_df[selected_users_df['pcode'] == user1][selected_columns]\n",
    "    data2 = selected_users_df[selected_users_df['pcode'] == user2][selected_columns]\n",
    "    \n",
    "    d_values = []\n",
    "    for column in selected_columns:\n",
    "        d = cohen_d(data1[column], data2[column])\n",
    "        if not np.isnan(d):\n",
    "            d_values.append(d)\n",
    "    # Compute mean absolute Cohen's D\n",
    "    if len(d_values) > 0:\n",
    "        mean_d = np.mean(d_values)\n",
    "    else:\n",
    "        mean_d = np.nan\n",
    "    cohen_d_matrix.loc[user1, user2] = mean_d\n",
    "\n",
    "# Convert to numeric\n",
    "cohen_d_matrix = cohen_d_matrix.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Replace infinite values\n",
    "cohen_d_matrix.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Ensure all values are non-negative (since we took absolute values)\n",
    "cohen_d_matrix = cohen_d_matrix.abs()\n",
    "\n",
    "# Step 4: Set the diagonal values explicitly to 0\n",
    "np.fill_diagonal(cohen_d_matrix.values, 0)\n",
    "\n",
    "# Step 5: Create a heatmap using Matplotlib's `pcolormesh()`\n",
    "data = cohen_d_matrix.values\n",
    "labels = cohen_d_matrix.index\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 12))\n",
    "\n",
    "# Define the color map and draw the heatmap using `pcolormesh`\n",
    "vmin, vmax = 0, 1  # Set fixed range for Cohen's D thresholds\n",
    "cmap = plt.get_cmap('RdYlBu_r')  # Vibrant color map for better contrast\n",
    "heatmap = ax.pcolormesh(data, cmap=cmap, edgecolors='k', linewidth=0.5, vmin=vmin, vmax=vmax)\n",
    "\n",
    "# Add color bar\n",
    "cbar = plt.colorbar(heatmap)\n",
    "cbar.set_label(\"Mean Absolute Cohen's D\", fontsize=14)\n",
    "\n",
    "# Set axis labels and ticks\n",
    "ax.set_xticks(np.arange(data.shape[1]) + 0.5)\n",
    "ax.set_yticks(np.arange(data.shape[0]) + 0.5)\n",
    "ax.set_xticklabels(labels, rotation=45, ha='right', fontsize=10)\n",
    "ax.set_yticklabels(labels, fontsize=10)\n",
    "\n",
    "# Annotate each cell with the value from `cohen_d_matrix`\n",
    "for i in range(data.shape[0]):\n",
    "    for j in range(data.shape[1]):\n",
    "        ax.text(j + 0.5, i + 0.5, f'{data[i, j]:.2f}', \n",
    "                ha='center', va='center', color='black', fontsize=12)\n",
    "\n",
    "# Set title and layout\n",
    "plt.title(\"Mean Absolute Cohen's D Between Users' Mobility Features\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9ebb3d",
   "metadata": {},
   "source": [
    "previous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03922eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cohens_d(group_a: np.ndarray, group_b: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Calculate Cohen's d for the difference between two groups.\n",
    "\n",
    "    Parameters:\n",
    "    - group_a: Array-like, data for group A.\n",
    "    - group_b: Array-like, data for group B.\n",
    "\n",
    "    Returns:\n",
    "    - d: Cohen's d value.\n",
    "    \"\"\"\n",
    "    # Calculate the means of the groups\n",
    "    mean_a = np.mean(group_a)\n",
    "    mean_b = np.mean(group_b)\n",
    "    \n",
    "    # Calculate the standard deviations of the groups\n",
    "    std_a = np.std(group_a, ddof=1)\n",
    "    std_b = np.std(group_b, ddof=1)\n",
    "    \n",
    "    # Calculate the pooled standard deviation\n",
    "    pooled_std = np.sqrt(((std_a ** 2) + (std_b ** 2)) / 2)\n",
    "    \n",
    "    # Handle the case where pooled_std is zero to avoid division by zero\n",
    "    if pooled_std == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # Calculate Cohen's d\n",
    "    d = (mean_a - mean_b) / pooled_std\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b70b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the index to have 'pcode' as a column\n",
    "features_df_reset = features_df_imputed.reset_index()\n",
    "\n",
    "# Display the first few rows to verify\n",
    "print(features_df_reset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8551911",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "\n",
    "# Define the list of selected users\n",
    "selected_users = ['P102', 'P020', 'P007', 'P120', 'P045']\n",
    "\n",
    "# Define the features to analyze\n",
    "features_to_analyze = [\n",
    "    \"MSG#CNT\",\n",
    "    \"CALL#CNT\",\n",
    "    \"CALL#DUR\"\n",
    "]\n",
    "\n",
    "# Initialize a list to store the results\n",
    "cohens_d_results = []\n",
    "\n",
    "# Iterate through each selected user\n",
    "for user in selected_users:\n",
    "    for feature in features_to_analyze:\n",
    "        # Group A: Data for the current user\n",
    "        group_a = features_df_reset[features_df_reset['pcode'] == user][feature].dropna().values\n",
    "        \n",
    "        # Group B: Data for all other users\n",
    "        group_b = features_df_reset[features_df_reset['pcode'] != user][feature].dropna().values\n",
    "        \n",
    "        # Compute Cohen's d\n",
    "        d = cohens_d(group_a, group_b)\n",
    "        \n",
    "        # Append the result\n",
    "        cohens_d_results.append({\n",
    "            'User': user,\n",
    "            'Feature': feature,\n",
    "            \"Cohen's d\": d\n",
    "        })\n",
    "\n",
    "# Convert the results to a DataFrame\n",
    "cohens_d_df = pd.DataFrame(cohens_d_results)\n",
    "\n",
    "# Display the Cohen's d results\n",
    "print(cohens_d_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ae70fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot the DataFrame\n",
    "cohens_d_pivot = cohens_d_df.pivot(index='User', columns='Feature', values=\"Cohen's d\")\n",
    "\n",
    "# Display the pivoted DataFrame\n",
    "print(cohens_d_pivot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b8a770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melt the pivoted DataFrame for easier plotting with seaborn\n",
    "cohens_d_melted = cohens_d_pivot.reset_index().melt(id_vars='User', var_name='Feature', value_name=\"Cohen's d\")\n",
    "\n",
    "# Set the plot size\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# Create a bar plot\n",
    "sns.barplot(x='Feature', y=\"Cohen's d\", hue='User', data=cohens_d_melted, palette='Set2')\n",
    "\n",
    "# Customize the plot\n",
    "plt.title(\"Cohen's d Effect Sizes Across Features for Selected Users\")\n",
    "plt.xlabel(\"Features\")\n",
    "plt.ylabel(\"Cohen's d\")\n",
    "plt.legend(title='User', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab41256c",
   "metadata": {},
   "source": [
    "# Hourly Visulization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56719e5a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Assuming all previous imports and preprocessing have been done\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Define the relevant features\n",
    "relevant_features = ['MSG#CNT', 'CALL#CNT', 'CALL#DUR']\n",
    "\n",
    "# Ensure these features exist in features_df_imputed\n",
    "missing_features = [feature for feature in relevant_features if feature not in features_df_imputed.columns]\n",
    "if missing_features:\n",
    "    logger.error(f\"The following required features are missing: {missing_features}\")\n",
    "    raise ValueError(f\"Missing features: {missing_features}\")\n",
    "\n",
    "# Extract the hourly data for PCA and t-SNE\n",
    "hourly_data = features_df_imputed[relevant_features].copy()\n",
    "hourly_data.reset_index(inplace=True)  # Make 'pcode' and 'window_end_time' columns\n",
    "\n",
    "# Check for any remaining missing values\n",
    "if hourly_data[relevant_features].isnull().any().any():\n",
    "    logger.warning(\"Missing values detected in hourly_data. Imputing with 0.\")\n",
    "    hourly_data[relevant_features] = hourly_data[relevant_features].fillna(0)\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(hourly_data[relevant_features])\n",
    "\n",
    "# Add scaled features to a new DataFrame\n",
    "scaled_df = pd.DataFrame(scaled_features, columns=relevant_features)\n",
    "scaled_df['pcode'] = hourly_data['pcode'].values\n",
    "scaled_df['window_end_time'] = hourly_data['window_end_time'].values\n",
    "\n",
    "# Prepare data for PCA and t-SNE\n",
    "X = scaled_df[relevant_features].values\n",
    "y = scaled_df['pcode'].values  # User labels\n",
    "\n",
    "# Conduct PCA\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "X_pca = pca.fit_transform(X)\n",
    "pca_df = pd.DataFrame(data=X_pca, columns=['PC1', 'PC2'])\n",
    "pca_df['User'] = y\n",
    "\n",
    "# Conduct t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30, max_iter=1000, init='pca')\n",
    "X_tsne = tsne.fit_transform(X)\n",
    "tsne_df = pd.DataFrame(data=X_tsne, columns=['TSNE1', 'TSNE2'])\n",
    "tsne_df['User'] = y\n",
    "\n",
    "# Visualize PCA Results\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.scatterplot(\n",
    "    x='PC1', y='PC2',\n",
    "    hue='User',\n",
    "    palette='tab10',\n",
    "    data=pca_df,\n",
    "    alpha=0.6\n",
    ")\n",
    "plt.title('PCA of Hourly User Data')\n",
    "plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.2f}% Variance)')\n",
    "plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.2f}% Variance)')\n",
    "plt.legend(title='User', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Visualize t-SNE Results\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.scatterplot(\n",
    "    x='TSNE1', y='TSNE2',\n",
    "    hue='User',\n",
    "    palette='tab10',\n",
    "    data=tsne_df,\n",
    "    alpha=0.6\n",
    ")\n",
    "plt.title('t-SNE of Hourly User Data')\n",
    "plt.xlabel('t-SNE Component 1')\n",
    "plt.ylabel('t-SNE Component 2')\n",
    "plt.legend(title='User', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Optional: Combined PCA and t-SNE Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "# PCA Plot\n",
    "sns.scatterplot(\n",
    "    x='PC1', y='PC2',\n",
    "    hue='User',\n",
    "    palette='tab10',\n",
    "    data=pca_df,\n",
    "    alpha=0.6,\n",
    "    ax=axes[0]\n",
    ")\n",
    "axes[0].set_title('PCA of Hourly User Data')\n",
    "axes[0].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.2f}% Variance)')\n",
    "axes[0].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.2f}% Variance)')\n",
    "\n",
    "# t-SNE Plot\n",
    "sns.scatterplot(\n",
    "    x='TSNE1', y='TSNE2',\n",
    "    hue='User',\n",
    "    palette='tab10',\n",
    "    data=tsne_df,\n",
    "    alpha=0.6,\n",
    "    ax=axes[1]\n",
    ")\n",
    "axes[1].set_title('t-SNE of Hourly User Data')\n",
    "axes[1].set_xlabel('t-SNE Component 1')\n",
    "axes[1].set_ylabel('t-SNE Component 2')\n",
    "\n",
    "# Adjust legends\n",
    "for ax in axes:\n",
    "    ax.legend(title='User', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc5faa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.dates as mdates\n",
    "import logging\n",
    "from typing import List\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"user_profile.log\"),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# Define the relevant features to plot\n",
    "relevant_features = ['MSG#CNT', 'CALL#CNT', 'CALL#DUR']\n",
    "\n",
    "# Ensure these features exist in features_df_imputed\n",
    "missing_features = [feature for feature in relevant_features if feature not in features_df_imputed.columns]\n",
    "if missing_features:\n",
    "    logger.error(f\"The following required features are missing: {missing_features}\")\n",
    "    raise ValueError(f\"Missing features: {missing_features}\")\n",
    "\n",
    "# Select 5 users for comparison\n",
    "unique_users = user_profiles.index.tolist()\n",
    "\n",
    "# Extract profiles of selected users (if needed for further analysis)\n",
    "selected_user_profiles = user_profiles.loc[selected_users]\n",
    "logger.info(f\"\\nSelected User Profiles:\\n{selected_user_profiles.head()}\")\n",
    "\n",
    "# Reset index to access 'pcode' and 'window_end_time' as columns\n",
    "features_df_imputed_reset = features_df_imputed.reset_index()\n",
    "\n",
    "# Filter the hourly data for selected users\n",
    "filtered_hourly_data = features_df_imputed_reset[features_df_imputed_reset['pcode'].isin(selected_users)].copy()\n",
    "\n",
    "# Sort the data by 'window_end_time' to ensure chronological order\n",
    "filtered_hourly_data.sort_values(by=['window_end_time'], inplace=True)\n",
    "\n",
    "# Ensure 'window_end_time' is in datetime format\n",
    "if not pd.api.types.is_datetime64_any_dtype(filtered_hourly_data['window_end_time']):\n",
    "    filtered_hourly_data['window_end_time'] = pd.to_datetime(filtered_hourly_data['window_end_time'])\n",
    "\n",
    "# Set up the color palette using a colorblind-friendly palette\n",
    "palette = sns.color_palette(\"Set2\", n_colors=len(selected_users))\n",
    "palette_dict = dict(zip(selected_users, palette))\n",
    "\n",
    "# Define plot size and style\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams.update({\n",
    "    'font.size': 14,\n",
    "    'axes.titlesize': 16,\n",
    "    'axes.labelsize': 14,\n",
    "    'legend.fontsize': 12,\n",
    "    'xtick.labelsize': 12,\n",
    "    'ytick.labelsize': 12\n",
    "})\n",
    "\n",
    "# Create a figure with separate subplots for each feature\n",
    "fig, axes = plt.subplots(len(relevant_features), 1, figsize=(15, 5 * len(relevant_features)), sharex=True)\n",
    "\n",
    "# Iterate through each feature and create a separate line plot\n",
    "for idx, feature in enumerate(relevant_features):\n",
    "    ax = axes[idx]\n",
    "    for user in selected_users:\n",
    "        user_data = filtered_hourly_data[filtered_hourly_data['pcode'] == user]\n",
    "        ax.plot(\n",
    "            user_data['window_end_time'],\n",
    "            user_data[feature],\n",
    "            label=user,\n",
    "            marker='o',\n",
    "            linestyle='-',\n",
    "            linewidth=2,\n",
    "            markersize=6,\n",
    "            color=palette_dict[user]\n",
    "        )\n",
    "    \n",
    "    # Set title and labels\n",
    "    ax.set_title(f'Hourly {feature} Across Users', fontsize=16, fontweight='bold')\n",
    "    ax.set_ylabel(f'{feature} Value', fontsize=14)\n",
    "    \n",
    "    # Format x-axis dates\n",
    "    ax.xaxis.set_major_locator(mdates.AutoDateLocator())\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d %H:%M'))\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Set y-axis limits based on feature\n",
    "    y_min = filtered_hourly_data[feature].min() * 0.95\n",
    "    y_max = filtered_hourly_data[feature].max() * 1.05\n",
    "    ax.set_ylim(y_min, y_max)\n",
    "    \n",
    "    # Add legend\n",
    "    ax.legend(title='User', loc='upper left', bbox_to_anchor=(1.02, 1))\n",
    "    \n",
    "    # Add grid for better readability\n",
    "    ax.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "# Set common x-axis label\n",
    "axes[-1].set_xlabel('Timestamp', fontsize=14)\n",
    "\n",
    "# Adjust layout to prevent overlapping elements\n",
    "plt.tight_layout(rect=[0, 0, 0.85, 1])  # Adjust rect to make space for legends\n",
    "\n",
    "# Add a main title for the entire figure\n",
    "plt.suptitle('Hourly Feature Trends Across Selected Users', fontsize=20, fontweight='bold', y=1.02)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b655157",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Assuming all previous imports and preprocessing have been done\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import logging\n",
    "\n",
    "# Configure logging (ensure this is done at the beginning of your script)\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"user_profile.log\"),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# Define the relevant features\n",
    "relevant_features = ['MSG#CNT', 'CALL#CNT', 'CALL#DUR']\n",
    "\n",
    "# Ensure these features exist in features_df_imputed\n",
    "missing_features = [feature for feature in relevant_features if feature not in features_df_imputed.columns]\n",
    "if missing_features:\n",
    "    logger.error(f\"The following required features are missing: {missing_features}\")\n",
    "    raise ValueError(f\"Missing features: {missing_features}\")\n",
    "\n",
    "# Select 5 users for comparison\n",
    "unique_users = user_profiles.index.tolist()\n",
    "\n",
    "# Extract profiles of selected users\n",
    "selected_user_profiles = user_profiles.loc[selected_users]\n",
    "logger.info(f\"\\nSelected User Profiles:\\n{selected_user_profiles.head()}\")\n",
    "\n",
    "# Reset index to access 'pcode' and 'window_end_time' as columns\n",
    "features_df_imputed_reset = features_df_imputed.reset_index()\n",
    "\n",
    "# Filter the hourly data for selected users\n",
    "filtered_hourly_data = features_df_imputed_reset[features_df_imputed_reset['pcode'].isin(selected_users)].copy()\n",
    "\n",
    "# Sort the data by 'window_end_time' to ensure chronological order\n",
    "filtered_hourly_data.sort_values(by=['window_end_time'], inplace=True)\n",
    "\n",
    "# Ensure 'window_end_time' is in datetime format\n",
    "if not pd.api.types.is_datetime64_any_dtype(filtered_hourly_data['window_end_time']):\n",
    "    filtered_hourly_data['window_end_time'] = pd.to_datetime(filtered_hourly_data['window_end_time'])\n",
    "\n",
    "# Extract the hourly data for PCA and t-SNE\n",
    "hourly_data = filtered_hourly_data[relevant_features].copy()\n",
    "hourly_data.reset_index(drop=True, inplace=True)  # Reset index after filtering\n",
    "\n",
    "# Check for any remaining missing values\n",
    "if hourly_data[relevant_features].isnull().any().any():\n",
    "    logger.warning(\"Missing values detected in hourly_data. Imputing with 0.\")\n",
    "    hourly_data[relevant_features] = hourly_data[relevant_features].fillna(0)\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(hourly_data[relevant_features])\n",
    "\n",
    "# Add scaled features to a new DataFrame\n",
    "scaled_df = pd.DataFrame(scaled_features, columns=relevant_features)\n",
    "scaled_df['pcode'] = filtered_hourly_data['pcode'].values\n",
    "scaled_df['window_end_time'] = filtered_hourly_data['window_end_time'].values\n",
    "\n",
    "# Prepare data for PCA and t-SNE\n",
    "X = scaled_df[relevant_features].values\n",
    "y = scaled_df['pcode'].values  # User labels\n",
    "\n",
    "# Conduct PCA\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "X_pca = pca.fit_transform(X)\n",
    "pca_df = pd.DataFrame(data=X_pca, columns=['PC1', 'PC2'])\n",
    "pca_df['User'] = y\n",
    "\n",
    "# Conduct t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30, max_iter=1000, init='pca')\n",
    "X_tsne = tsne.fit_transform(X)\n",
    "tsne_df = pd.DataFrame(data=X_tsne, columns=['TSNE1', 'TSNE2'])\n",
    "tsne_df['User'] = y\n",
    "\n",
    "# Visualize PCA Results\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.scatterplot(\n",
    "    x='PC1', y='PC2',\n",
    "    hue='User',\n",
    "    palette='tab10',\n",
    "    data=pca_df,\n",
    "    alpha=0.7,\n",
    "    edgecolor='k',\n",
    "    s=100\n",
    ")\n",
    "plt.title('PCA of Hourly User Data', fontsize=18, fontweight='bold')\n",
    "plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.2f}% Variance)', fontsize=14)\n",
    "plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.2f}% Variance)', fontsize=14)\n",
    "plt.legend(title='User', bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=12, title_fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Visualize t-SNE Results\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.scatterplot(\n",
    "    x='TSNE1', y='TSNE2',\n",
    "    hue='User',\n",
    "    palette='tab10',\n",
    "    data=tsne_df,\n",
    "    alpha=0.7,\n",
    "    edgecolor='k',\n",
    "    s=100\n",
    ")\n",
    "plt.title('t-SNE of Hourly User Data', fontsize=18, fontweight='bold')\n",
    "plt.xlabel('t-SNE Component 1', fontsize=14)\n",
    "plt.ylabel('t-SNE Component 2', fontsize=14)\n",
    "plt.legend(title='User', bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=12, title_fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Optional: Combined PCA and t-SNE Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(24, 10))\n",
    "\n",
    "# PCA Plot\n",
    "sns.scatterplot(\n",
    "    x='PC1', y='PC2',\n",
    "    hue='User',\n",
    "    palette='tab10',\n",
    "    data=pca_df,\n",
    "    alpha=0.7,\n",
    "    edgecolor='k',\n",
    "    s=100,\n",
    "    ax=axes[0]\n",
    ")\n",
    "axes[0].set_title('PCA of Hourly User Data', fontsize=18, fontweight='bold')\n",
    "axes[0].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.2f}% Variance)', fontsize=14)\n",
    "axes[0].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.2f}% Variance)', fontsize=14)\n",
    "axes[0].legend(title='User', bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=12, title_fontsize=14)\n",
    "\n",
    "# t-SNE Plot\n",
    "sns.scatterplot(\n",
    "    x='TSNE1', y='TSNE2',\n",
    "    hue='User',\n",
    "    palette='tab10',\n",
    "    data=tsne_df,\n",
    "    alpha=0.7,\n",
    "    edgecolor='k',\n",
    "    s=100,\n",
    "    ax=axes[1]\n",
    ")\n",
    "axes[1].set_title('t-SNE of Hourly User Data', fontsize=18, fontweight='bold')\n",
    "axes[1].set_xlabel('t-SNE Component 1', fontsize=14)\n",
    "axes[1].set_ylabel('t-SNE Component 2', fontsize=14)\n",
    "axes[1].legend(title='User', bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=12, title_fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3778e26",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.dates as mdates\n",
    "import logging\n",
    "from typing import List\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "# Configure logging (ensure this is done at the beginning of your script)\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"user_profile.log\"),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# Define the relevant features to plot\n",
    "relevant_features = ['MSG#CNT', 'CALL#CNT', 'CALL#DUR']\n",
    "\n",
    "# Ensure these features exist in features_df_imputed\n",
    "missing_features = [feature for feature in relevant_features if feature not in features_df_imputed.columns]\n",
    "if missing_features:\n",
    "    logger.error(f\"The following required features are missing: {missing_features}\")\n",
    "    raise ValueError(f\"Missing features: {missing_features}\")\n",
    "\n",
    "# Select 5 users for comparison\n",
    "unique_users = user_profiles.index.tolist()\n",
    "\n",
    "# Extract profiles of selected users\n",
    "selected_user_profiles = user_profiles.loc[selected_users]\n",
    "logger.info(f\"\\nSelected User Profiles:\\n{selected_user_profiles.head()}\")\n",
    "\n",
    "# Reset index to access 'pcode' and 'window_end_time' as columns\n",
    "features_df_imputed_reset = features_df_imputed.reset_index()\n",
    "\n",
    "# Filter the hourly data for selected users\n",
    "filtered_hourly_data = features_df_imputed_reset[features_df_imputed_reset['pcode'].isin(selected_users)].copy()\n",
    "\n",
    "# Sort the data by 'window_end_time' to ensure chronological order\n",
    "filtered_hourly_data.sort_values(by=['window_end_time'], inplace=True)\n",
    "\n",
    "# Ensure 'window_end_time' is in datetime format\n",
    "if not pd.api.types.is_datetime64_any_dtype(filtered_hourly_data['window_end_time']):\n",
    "    filtered_hourly_data['window_end_time'] = pd.to_datetime(filtered_hourly_data['window_end_time'])\n",
    "\n",
    "# ------------------ Daily Aggregation Step ------------------\n",
    "\n",
    "# Create a 'date' column by extracting the date from 'window_end_time'\n",
    "filtered_hourly_data['date'] = filtered_hourly_data['window_end_time'].dt.date\n",
    "\n",
    "# Group by 'pcode' and 'date', then calculate the mean for each feature\n",
    "daily_aggregated_data = filtered_hourly_data.groupby(['pcode', 'date'])[relevant_features].mean().reset_index()\n",
    "\n",
    "# Convert 'date' back to datetime for plotting\n",
    "daily_aggregated_data['date'] = pd.to_datetime(daily_aggregated_data['date'])\n",
    "\n",
    "# ------------------ End of Aggregation ------------------\n",
    "\n",
    "# Log the head of the aggregated data for verification\n",
    "logger.info(f\"\\nDaily Aggregated User Profiles:\\n{daily_aggregated_data.head()}\")\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(daily_aggregated_data[relevant_features])\n",
    "\n",
    "# Add scaled features to a new DataFrame\n",
    "scaled_df = pd.DataFrame(scaled_features, columns=relevant_features)\n",
    "scaled_df['pcode'] = daily_aggregated_data['pcode'].values\n",
    "scaled_df['date'] = daily_aggregated_data['date'].values\n",
    "\n",
    "# Prepare data for LDA\n",
    "X = scaled_df[relevant_features].values\n",
    "y = scaled_df['pcode'].values  # User labels\n",
    "\n",
    "# Conduct LDA\n",
    "lda = LDA(n_components=2)\n",
    "X_lda = lda.fit_transform(X, y)\n",
    "lda_df = pd.DataFrame(data=X_lda, columns=['LD1', 'LD2'])\n",
    "lda_df['User'] = y\n",
    "\n",
    "# ------------------ Visualization: LDA Results ------------------\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.scatterplot(\n",
    "    x='LD1', y='LD2',\n",
    "    hue='User',\n",
    "    palette='tab10',\n",
    "    data=lda_df,\n",
    "    alpha=0.7,\n",
    "    edgecolor='k',\n",
    "    s=100\n",
    ")\n",
    "plt.title('LDA of Daily Aggregated User Data', fontsize=18, fontweight='bold')\n",
    "plt.xlabel(f'LD1', fontsize=14)\n",
    "plt.ylabel(f'LD2', fontsize=14)\n",
    "plt.legend(title='User', bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=12, title_fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302b24e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ Visualization: Small Multiples Line Plots ------------------\n",
    "\n",
    "# Set up the color palette using a colorblind-friendly palette\n",
    "palette = sns.color_palette(\"Set2\", n_colors=len(selected_users))\n",
    "palette_dict = dict(zip(selected_users, palette))\n",
    "\n",
    "# Define plot size and style\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams.update({\n",
    "    'font.size': 14,\n",
    "    'axes.titlesize': 16,\n",
    "    'axes.labelsize': 14,\n",
    "    'legend.fontsize': 12,\n",
    "    'xtick.labelsize': 12,\n",
    "    'ytick.labelsize': 12\n",
    "})\n",
    "\n",
    "# Determine the grid size for subplots (e.g., 1 row x 3 columns for 3 features)\n",
    "cols = 3\n",
    "rows = 1  # Single row since there are 3 features\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(20, 6), sharey=False)\n",
    "\n",
    "# Flatten axes array for easy iteration (in case of multiple rows)\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Iterate through each feature and create a separate line plot\n",
    "for idx, feature in enumerate(relevant_features):\n",
    "    ax = axes[idx]\n",
    "    for user in selected_users:\n",
    "        user_data = daily_aggregated_data[daily_aggregated_data['pcode'] == user]\n",
    "        ax.plot(\n",
    "            user_data['date'],\n",
    "            user_data[feature],\n",
    "            label=user,\n",
    "            marker='o',\n",
    "            linestyle='-',\n",
    "            linewidth=2,\n",
    "            markersize=6,\n",
    "            color=palette_dict[user]\n",
    "        )\n",
    "    \n",
    "    # Set title and labels\n",
    "    ax.set_title(f'Daily Mean {feature} Across Users', fontsize=16, fontweight='bold')\n",
    "    ax.set_ylabel(f'{feature} Mean Value', fontsize=14)\n",
    "    \n",
    "    # Format x-axis dates for better readability\n",
    "    ax.xaxis.set_major_locator(mdates.AutoDateLocator())\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Set y-axis limits based on feature to ensure consistency\n",
    "    y_min = daily_aggregated_data[feature].min() * 0.95\n",
    "    y_max = daily_aggregated_data[feature].max() * 1.05\n",
    "    ax.set_ylim(y_min, y_max)\n",
    "    \n",
    "    # Add legend outside the plot area to prevent overlap\n",
    "    ax.legend(title='User', loc='upper left', bbox_to_anchor=(1.02, 1))\n",
    "    \n",
    "    # Add grid for better readability\n",
    "    ax.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "# Set common x-axis label\n",
    "axes[-1].set_xlabel('Date', fontsize=14)\n",
    "\n",
    "# Adjust layout to prevent overlapping elements and make space for legends\n",
    "plt.tight_layout(rect=[0, 0, 0.85, 1])  # Adjust rect to make space for legends\n",
    "\n",
    "# Add a main title for the entire figure\n",
    "plt.suptitle('Daily Aggregated Feature Trends Across Selected Users', fontsize=20, fontweight='bold', y=1.02)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# ------------------ End of Small Multiples Visualization ------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbbdbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ Visualization: Separate Line Plots for Each Feature ------------------\n",
    "\n",
    "# Set up the color palette using a colorblind-friendly palette\n",
    "palette = sns.color_palette(\"Set2\", n_colors=len(selected_users))\n",
    "palette_dict = dict(zip(selected_users, palette))\n",
    "\n",
    "# Define plot style\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams.update({\n",
    "    'font.size': 14,\n",
    "    'axes.titlesize': 16,\n",
    "    'axes.labelsize': 14,\n",
    "    'legend.fontsize': 12,\n",
    "    'xtick.labelsize': 12,\n",
    "    'ytick.labelsize': 12\n",
    "})\n",
    "\n",
    "# Iterate through each feature and create a separate line plot\n",
    "for feature in relevant_features:\n",
    "    # Create a new figure for each feature\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    for user in selected_users:\n",
    "        user_data = daily_aggregated_data[daily_aggregated_data['pcode'] == user]\n",
    "        plt.plot(\n",
    "            user_data['date'],\n",
    "            user_data[feature],\n",
    "            label=user,\n",
    "            marker='o',\n",
    "            linestyle='-',\n",
    "            linewidth=2,\n",
    "            markersize=6,\n",
    "            color=palette_dict[user]\n",
    "        )\n",
    "\n",
    "    # Set title and labels\n",
    "    plt.title(f'Daily Mean {feature} Across Users', fontsize=16, fontweight='bold')\n",
    "    plt.ylabel(f'{feature} Mean Value', fontsize=14)\n",
    "    plt.xlabel('Date', fontsize=14)\n",
    "\n",
    "    # Format x-axis dates for better readability\n",
    "    plt.gca().xaxis.set_major_locator(mdates.AutoDateLocator())\n",
    "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    # Set y-axis limits based on feature to ensure consistency\n",
    "    y_min = daily_aggregated_data[feature].min() * 0.95\n",
    "    y_max = daily_aggregated_data[feature].max() * 1.05\n",
    "    plt.ylim(y_min, y_max)\n",
    "\n",
    "    # Add legend outside the plot area to prevent overlap\n",
    "    plt.legend(title='User', loc='upper left', bbox_to_anchor=(1.02, 1))\n",
    "\n",
    "    # Add grid for better readability\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "    # Adjust layout to prevent overlapping elements\n",
    "    plt.tight_layout(rect=[0, 0, 0.85, 1])\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416d30b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
